{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Forecasting using Transformers\n",
    "\n",
    "In this notebook we implement a Transformer model to forecast stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/a/64438413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import glob\n",
    "import inspect\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set() # Use seaborn themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "This section contains code that is modifies output path locations, random seed, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = Path('~/ml/datasets').expanduser()\n",
    "if not DATASET_ROOT.exists(): raise ValueError(f\"Dataset root directory does not exist at {DATASET_ROOT}\")\n",
    "PROJECT_ROOT = Path('~/ml/ece_6524/final_project').expanduser()\n",
    "CHECKPOINT_ROOT = PROJECT_ROOT / 'checkpoints'\n",
    "IMAGE_ROOT = PROJECT_ROOT / 'images'\n",
    "TABLE_ROOT = PROJECT_ROOT / 'tables'\n",
    "\n",
    "# Ensure some directories exist.\n",
    "PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "TABLE_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds.\n",
    "SEED = 0\n",
    "tf.random.set_seed(SEED) # Only this works on ARC (since tensorflow==2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging (useful for ARC systems).\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) # Must be lowest of all handlers listed below.\n",
    "while logger.hasHandlers(): logger.removeHandler(logger.handlers[0]) # Clear all existing handlers.\n",
    "\n",
    "# Custom log formatting.\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "# Log to STDOUT (uses default formatting).\n",
    "sh = logging.StreamHandler(stream=sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "# Set Tensorflow logging level.\n",
    "tf.get_logger().setLevel('ERROR') # 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API configured\n"
     ]
    }
   ],
   "source": [
    "# Request user for Kaggle login if JSON file does not exist.\n",
    "kaggle_config_file = Path(\"~/.kaggle/kaggle.json\").expanduser()\n",
    "if not kaggle_config_file.exists() and os.environ.get(\"KAGGLE_USERNAME\", None) is None and os.environ.get(\"KAGGLE_KEY\", None) is None:\n",
    "    import json\n",
    "    import getpass\n",
    "    entry = getpass.getpass(prompt=\"Please enter your Kaggle username or JSON blob: \")\n",
    "    try:\n",
    "        blob = json.loads(entry)\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = blob['username']\n",
    "        os.environ[\"KAGGLE_KEY\"] = blob['key']\n",
    "    except:\n",
    "        api_key = getpass.getpass(prompt=\"Please enter your Kaggle API KEY: \")\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = entry\n",
    "        os.environ[\"KAGGLE_KEY\"] = api_key\n",
    "else:\n",
    "    logger.info('Kaggle API configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "# List all GPUs visible to TensorFlow.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "logger.info(f\"Num GPUs Available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    logger.info(f\"Name: {gpu.name}, Type: {gpu.device_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huge Stock Market Dataset from Kaggle\n",
    "\n",
    "https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HugeStockMarketDataset:\n",
    "    \"\"\"Wrapper for Huge Stock Market Dataset by Boris Marjanovic on Kaggle.\n",
    "\n",
    "    Source URL is: https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs/version/3\n",
    "\n",
    "    This class can be used like a Python dictionary, where keys are the stock/etf names, and values are\n",
    "    `pandas.DataFrame` objects corresponding to that stock/etf.\n",
    "    \"\"\"\n",
    "    root = 'HugeStockMarketDataset'\n",
    "\n",
    "    def __init__(self, \n",
    "        path: str,\n",
    "        files: list = None,\n",
    "        quiet: bool = False,\n",
    "        exclude_stocks: bool = False,\n",
    "        exclude_etfs: bool = False,\n",
    "        usecols: list[str] = ['Date','Open','High','Low','Close','Volume','OpenInt'],\n",
    "        ):\n",
    "        self.exclude_stocks = exclude_stocks\n",
    "        self.exclude_etfs = exclude_etfs\n",
    "        self.usecols = usecols\n",
    "        self._index = {}\n",
    "\n",
    "        # Download the dataset if necessary.\n",
    "        newpath = Path(path).expanduser()/self.root\n",
    "        if not newpath.exists():\n",
    "            self.download(newpath, files, quiet=quiet)\n",
    "        else:\n",
    "            self.path = newpath\n",
    "            self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"Creates an internal index of stocks and ETFs for lookup.\"\"\"\n",
    "\n",
    "        # Helper function to index a folder of files.\n",
    "        def _index_folder(dir: Path):\n",
    "            for file in glob.iglob(str(dir/'*.txt'), recursive=True):\n",
    "                filename = Path(file).name\n",
    "                product_name = filename.split('.', maxsplit=1)[0]\n",
    "                self._index[product_name] = file\n",
    "\n",
    "        # Index all stocks.\n",
    "        if not self.exclude_stocks:\n",
    "            _index_folder(self.path/'Stocks')\n",
    "        \n",
    "        # Index all ETFs.\n",
    "        if not self.exclude_etfs:\n",
    "            _index_folder(self.path/'ETFs')\n",
    "\n",
    "    def download(self, path: str, files: list = None, quiet: bool = True):\n",
    "        \"\"\"Downloads the dataset from Kaggle.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to place the download.\n",
    "            files (list, optional): Subset list of files to download instead of entire dataset. Defaults to None.\n",
    "            quiet (bool, optional): Suppress verbose output. Defaults to True.\n",
    "        \"\"\"\n",
    "        import kaggle\n",
    "        kaggle_dataset = 'borismarjanovic/price-volume-data-for-all-us-stocks-etfs'\n",
    "        kaggle.api.authenticate()\n",
    "\n",
    "        # Save the new downloaded path.\n",
    "        self.path = Path(path).expanduser()\n",
    "\n",
    "        # Specific file list was given.\n",
    "        if files is not None:\n",
    "            for f in files:\n",
    "                kaggle.api.dataset_download_file(\n",
    "                    dataset=kaggle_dataset,\n",
    "                    file_name=f,\n",
    "                    path=path/f,\n",
    "                    quiet=quiet,\n",
    "                )\n",
    "        # Download all files.\n",
    "        else:\n",
    "            kaggle.api.dataset_download_files(\n",
    "                dataset=kaggle_dataset,\n",
    "                path=path,\n",
    "                unzip=True,\n",
    "                quiet=quiet,\n",
    "            )\n",
    "\n",
    "        # Force rebuild the index after downloading.\n",
    "        logger.info(\"Building file index\")\n",
    "        self._build_index()\n",
    "\n",
    "    def get_dataframe(self, \n",
    "        key: str,\n",
    "        **kwargs,\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"Obtain historical data for stock or ETF in a pandas dataframe.\n",
    "\n",
    "        Optional keyword arguments are passed directly to `pandas.read_csv` function.\n",
    "\n",
    "        Args:\n",
    "            key (str): The identifier for the stock or ETF.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Historical data.\n",
    "        \"\"\"\n",
    "        return pd.read_csv(self._index[key], **kwargs)\n",
    "\n",
    "    #### Dictionary Override ######\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, str):\n",
    "            return self.get_dataframe(key, usecols=self.usecols)\n",
    "        elif isinstance(key, list):\n",
    "            return [self.get_dataframe(asset, usecols=self.usecols) for asset in key]\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        del self._index[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._index)\n",
    "\n",
    "    def items(self):\n",
    "        for key in self:\n",
    "            yield key, self.get_dataframe(key, usecols=self.usecols)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._index)\n",
    "\n",
    "    def keys(self):\n",
    "        \"\"\"Returns a list of all downloaded stocks and ETFs.\"\"\"\n",
    "        return self._index.keys()\n",
    "\n",
    "    ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration: https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing\n",
    "class WindowGenerator:\n",
    "    def __init__(self,\n",
    "        in_seq_len: int,\n",
    "        out_seq_len: int,\n",
    "        shift: int,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        in_feat: list[str] = None,\n",
    "        out_feat: list[str] = None,\n",
    "        batch_size: int = 32,\n",
    "        shuffle: bool = True,\n",
    "        ):\n",
    "        \"\"\"Constructs sliding windows of sequential data.\n",
    "\n",
    "        Data must already be split into train/val/test subsets,\n",
    "        and provided as `pandas.DataFrame` objects.\n",
    "\n",
    "        Args:\n",
    "            in_seq_len (int): Input sequence length.\n",
    "            out_seq_len (int): Output (target) sequence length.\n",
    "            shift (int): Number of indices to skip between elements when traversing window.\n",
    "            train_df (pd.DataFrame): Training data frame.\n",
    "            val_df (pd.DataFrame): Validation data frame.\n",
    "            test_df (pd.DataFrame): Testing data frame.\n",
    "            in_feat (list[str], optional): Desired subset of input features for window. Defaults to None.\n",
    "            out_feat (list[str], optional): Desired subset of output features for window. Defaults to None.\n",
    "            batch_size (int, optional): Batch size. Defaults to 32.\n",
    "            shuffle (bool, optional): Shuffle windows prior to batching. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Preserve dataframes.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Preserve sequence information.\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.shift = shift\n",
    "        self.total_window_len = in_seq_len + shift\n",
    "\n",
    "        # Setup indexing slices for window extraction.\n",
    "        self.in_slice = slice(0, self.in_seq_len)\n",
    "        self.out_slice = slice(self.total_window_len - self.out_seq_len, None)\n",
    "        self.in_idx = np.arange(self.total_window_len)[self.in_slice]\n",
    "        self.out_idx = np.arange(self.total_window_len)[self.out_slice]\n",
    "\n",
    "        # Setup train/val/test column extractors.\n",
    "        self.col2idx = {name: i for i, name in enumerate(train_df.columns)}\n",
    "        if in_feat is not None:\n",
    "            self.in_feat = in_feat\n",
    "            self.in_col_idx = [self.col2idx[col] for col in in_feat]\n",
    "        else:\n",
    "            self.in_col_idx = list(range(len(train_df.columns)))\n",
    "            self.in_feat = [train_df.columns[i] for i in self.in_col_idx]\n",
    "        if out_feat is not None:\n",
    "            self.out_feat = out_feat\n",
    "            self.out_col_idx = [self.col2idx[col] for col in out_feat]\n",
    "        else:\n",
    "            self.out_col_idx = list(range(len(train_df.columns)))\n",
    "            self.out_feat = [train_df.columns[i] for i in self.out_col_idx]\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"String representation of class.\"\"\"\n",
    "        return '\\n'.join([\n",
    "            f\"Total window length: {self.total_window_len}\",\n",
    "            f\"Input indices: {self.in_idx}\",\n",
    "            f\"Output indices: {self.out_idx}\",\n",
    "            f\"Input features: {self.in_feat}\",\n",
    "            f\"Output features: {self.out_feat}\",\n",
    "        ])\n",
    "\n",
    "    def split_window(self, \n",
    "        window: tf.Tensor, # window shape is (batch, seq, feat)\n",
    "        ) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"Splits a single window of data into input/output seqments.\n",
    "\n",
    "        Args:\n",
    "            window (tf.Tensor): Tensor of window data with shape (batch, seq, feat).\n",
    "\n",
    "        Returns:\n",
    "            tuple[tf.Tensor, tf.Tensor]: 2-tuple of input/output data segments, where the shapes are:\n",
    "                - Input window: (batch, in_seq_len, in_feat)\n",
    "                - Output window: (batch, out_seq_len, out_feat)\n",
    "        \"\"\"\n",
    "        # Decompose input/output sequence from given input window.\n",
    "        in_seq = tf.stack([window[:, self.in_slice, i] for i in self.in_col_idx], axis=-1)\n",
    "        out_seq = tf.stack([window[:, self.out_slice, i] for i in self.out_col_idx], axis=-1)\n",
    "\n",
    "        # Set shape for input/output sequences.\n",
    "        # Note that dimensions set to `None` are not updated.\n",
    "        in_seq = tf.ensure_shape(in_seq, (None, self.in_seq_len, None))\n",
    "        out_seq = tf.ensure_shape(out_seq, (None, self.out_seq_len, None))\n",
    "\n",
    "        return in_seq, out_seq\n",
    "\n",
    "    def make_dataset(self, \n",
    "        df: pd.DataFrame,\n",
    "        batch_size: int = 32,\n",
    "        shuffle: bool = True,\n",
    "        ) -> tf.data.Dataset:\n",
    "        \"\"\"Construct a TensorFlow Dataset from given input data frame.\n",
    "\n",
    "        Datasets load tuples of batched input/output windows with shapes:\n",
    "            - Input window: (batch, in_seq_len, in_feat)\n",
    "            - Output window: (batch, out_seq_len, out_feat)\n",
    "\n",
    "        Note that output windows are generally target sequences.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Source data frame.\n",
    "            batch_size (int, optional): Batch size. Defaults to 32.\n",
    "            shuffle (bool, optional): Shuffle windows prior to batching. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            tf.data.Dataset: Dataset object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert data frame into numpy matrix.\n",
    "        data = df.to_numpy()\n",
    "\n",
    "        # Convert data matrix into TensorFlow dataset.\n",
    "        dataset = keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_len,\n",
    "            sequence_stride=self.shift,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        # Pipe the raw dataset into the window splitting function.\n",
    "        dataset = dataset.map(self.split_window)\n",
    "\n",
    "        # Return the dataset.\n",
    "        return dataset\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        \"\"\"Training dataset.\"\"\"\n",
    "        return self.make_dataset(\n",
    "            df=self.train_df,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        \"\"\"Validation dataset.\"\"\"\n",
    "        return self.make_dataset(\n",
    "            df=self.val_df,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        \"\"\"Testing dataset.\"\"\"\n",
    "        return self.make_dataset(\n",
    "            df=self.test_df,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_dataset(\n",
    "    asset: str,\n",
    "    in_seq_len: int,\n",
    "    out_seq_len: int,\n",
    "    shift: int,\n",
    "    split: tuple[float, float, float], # must sum to 1.\n",
    "    in_feat: list[str] = ['Open','High','Low','Close','Volume'],\n",
    "    out_feat: list[str] = ['Open','High','Low','Close','Volume'],\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "    normalize: bool = True,\n",
    "    return_datasets: bool = True,\n",
    "    return_df: bool = False,\n",
    "    return_window: bool = False,\n",
    "    ) -> None|tuple:\n",
    "    \"\"\"Load the history for a single stock within the Huge Stock Market Dataset.\n",
    "\n",
    "    Args:\n",
    "        asset (str): Name of stock or ETF to use.\n",
    "        in_seq_len (int): Input sequence length for window.\n",
    "        out_seq_len (int): Output (target) sequence length for window.\n",
    "        shift (int): Number of indices to skip when generating window.\n",
    "        split (tuple[float, float, float]): Tuple of `(train, val, test)` splits. Note that these must sum to `1`.\n",
    "        in_feat (list[str], optional): Desired subset of input features for window. Defaults to `['Open','High','Low','Close','Volume']`.\n",
    "        out_feat (list[str], optional): Desired subset of output features for window. Defaults to `['Open','High','Low','Close','Volume']`.\n",
    "        batch_size (int, optional): Batch size. Defaults to 32.\n",
    "        shuffle (bool, optional): Shuffle windows prior to batching. Defaults to True.\n",
    "        normalize (bool, optional): Normalize the data using mean/std method. Defaults to `True`.\n",
    "        return_datasets (bool, optional): Return train/val/test datasets. Defaults to `True`.\n",
    "        return_df (bool, optional): Return original data frame. Defaults to `False`.\n",
    "        return_window (bool, optional): Return window generator. Defaults to `False`.\n",
    "\n",
    "    Returns:\n",
    "        None|tuple: Several return options:\n",
    "            - `return_datasets=False`, `return_df=False`, `return_window=False`: Returns None.\n",
    "            - `return_datasets=True`, `return_df=False`, `return_window=False`: Returns tuple of `(train, val, test)` datasets.\n",
    "            - `return_datasets=False`, `return_df=True`, `return_window=False`: Returns the original dataset `df`.\n",
    "            - `return_datasets=False`, `return_df=False`, `return_window=True`: Returns the window generator `windowgen`.\n",
    "            - `return_datasets=True`, `return_df=True`, `return_window=False`: Returns tuple of `((train, val, test), df)`\n",
    "            - `return_datasets=True`, `return_df=False`, `return_window=True`: Returns tuple of `((train, val, test), windowgen)`\n",
    "            - `return_datasets=False`, `return_df=True`, `return_window=True`: Returns tuple of `(windowgen, df)`\n",
    "            - `return_datasets=True`, `return_df=True`, `return_window=True`: Returns tuple of `((train, val, test), df, windowgen)`\n",
    "    \"\"\"\n",
    "    np.testing.assert_almost_equal(sum(split), 1., err_msg='Split must sum to 1.')\n",
    "    train_split, _, test_split = split\n",
    "\n",
    "    # Get dataframe for desired asset.\n",
    "    dataset = HugeStockMarketDataset(DATASET_ROOT, usecols=['Date','Open','High','Low','Close','Volume'])\n",
    "    df = dataset[asset]\n",
    "\n",
    "    # Split dataframe into train/val/test dataframes.\n",
    "    # Inspiration: https://www.tensorflow.org/tutorials/structured_data/time_series#split_the_data\n",
    "    n = len(df.index) # Total number of data records.\n",
    "    df_train = df[:int(n*train_split)].copy()\n",
    "    df_val = df[int(n*train_split):int(n*(1-test_split))].copy()\n",
    "    df_test = df[int(n*(1-test_split)):].copy()\n",
    "\n",
    "    # Drop the 'Date' column, so that we only use the floating-point columns.\n",
    "    df_train.drop(columns=['Date'], inplace=True, errors='ignore')\n",
    "    df_val.drop(columns=['Date'], inplace=True, errors='ignore')\n",
    "    df_test.drop(columns=['Date'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Normalize the datasets using train-data statistics.\n",
    "    # Note that only the training data is used for statistics.\n",
    "    # Inspiration: https://www.tensorflow.org/tutorials/structured_data/time_series#normalize_the_data\n",
    "    if normalize:\n",
    "        train_mean = df_train.mean()\n",
    "        train_std = df_train.std()\n",
    "        df_train = (df_train - train_mean)/train_std\n",
    "        df_val = (df_val - train_mean)/train_std\n",
    "        df_test = (df_test - train_mean)/train_std\n",
    "\n",
    "    # Build window generator for datasets.\n",
    "    w = WindowGenerator(\n",
    "        in_seq_len=in_seq_len,\n",
    "        out_seq_len=out_seq_len,\n",
    "        shift=shift,\n",
    "        train_df=df_train,\n",
    "        val_df=df_val,\n",
    "        test_df=df_test,\n",
    "        in_feat=in_feat,\n",
    "        out_feat=out_feat,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "    # Extract elements for result list.\n",
    "    result = []\n",
    "    # Extract train/val/test datasets and return.\n",
    "    if return_datasets:\n",
    "        result.append((w.train, w.val, w.test))\n",
    "    # Extract the data frame.\n",
    "    if return_df:\n",
    "        result.append(df)\n",
    "    # Extract the window generator.\n",
    "    if return_window:\n",
    "        result.append(w)\n",
    "\n",
    "    # Return None if nothing was selected to return.\n",
    "    if len(result) == 0:\n",
    "        return None\n",
    "    # Return single element.\n",
    "    elif len(result) == 1:\n",
    "        return result[0]\n",
    "    # Return entire tuple of results.\n",
    "    else:\n",
    "        return tuple(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets actually load the dataset. In this case, we're only looking at the `aapl` stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train.element_spec=(TensorSpec(shape=(None, 128, 5), dtype=tf.float64, name=None), TensorSpec(shape=(None, 1, 5), dtype=tf.float64, name=None))\n",
      "dataset_val.element_spec=(TensorSpec(shape=(None, 128, 5), dtype=tf.float64, name=None), TensorSpec(shape=(None, 1, 5), dtype=tf.float64, name=None))\n",
      "dataset_test.element_spec=(TensorSpec(shape=(None, 128, 5), dtype=tf.float64, name=None), TensorSpec(shape=(None, 1, 5), dtype=tf.float64, name=None))\n"
     ]
    }
   ],
   "source": [
    "asset = 'aapl'\n",
    "(dataset_train, dataset_val, dataset_test), df = load_stock_dataset(\n",
    "    asset=asset,\n",
    "    in_seq_len=128,\n",
    "    out_seq_len=1,\n",
    "    shift=1,\n",
    "    split=(0.7, 0.2, 0.1),\n",
    "    return_datasets=True,\n",
    "    return_df=True,\n",
    ")\n",
    "print(f\"{dataset_train.element_spec=}\")\n",
    "print(f\"{dataset_val.element_spec=}\")\n",
    "print(f\"{dataset_test.element_spec=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the closing price and volume for the chosen stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(15,7))\n",
    "# fig.suptitle(f'{asset.upper()} Close Price and Volume')\n",
    "\n",
    "# n = len(df.index)//8 # Number of date record ticks on X-axis.\n",
    "\n",
    "# ax = fig.add_subplot(211)\n",
    "# sns.lineplot(x='Date', y='Close', data=df, ax=ax)\n",
    "# ax.set_xticks(range(0, df.shape[0], n))\n",
    "# ax.set_xticklabels(df['Date'].loc[::n])\n",
    "\n",
    "# ax = fig.add_subplot(212)\n",
    "# sns.lineplot(x='Date', y='Volume', data=df, ax=ax)\n",
    "# ax.set_xticks(range(0, df.shape[0], n))\n",
    "# ax.set_xticklabels(df['Date'].loc[::n]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time2Vec Embedding\n",
    "\n",
    "https://arxiv.org/abs/1907.05321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vec(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim: int, activation: str = 'sin', **kwargs):\n",
    "        \"\"\"Vector embedding representation of time.\n",
    "\n",
    "        Based on the original concept proposed by Kazemi et al., 2019 (https://arxiv.org/abs/1907.05321).\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Length of the time embedding vector.\n",
    "            activation (str, optional): Periodic activation function. Possible values are ['sin', 'cos']. Defaults to 'sin'.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.activation = activation.lower() # Convert to lower-case.\n",
    "\n",
    "        # Set periodic activation function.\n",
    "        if self.activation.startswith('sin'):\n",
    "            self.activation_func = tf.sin\n",
    "        elif self.activation.startswith('cos'):\n",
    "            self.activation_func = tf.cos\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported periodic activation function \"{activation}\"')\n",
    "\n",
    "    def build(self, input_shape: list[int]):\n",
    "\n",
    "        # Weight and bias term for linear portion (i = 0)\n",
    "        # of embedding.\n",
    "        self.w_linear = self.add_weight(\n",
    "            name='w_linear',\n",
    "            # shape=(input_shape[1], 1,),\n",
    "            shape=(input_shape[-1], 1,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b_linear = self.add_weight(\n",
    "            name='b_linear',\n",
    "            # shape=(input_shape[1], 1,),\n",
    "            shape=(input_shape[1], 1,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Weight and bias terms for the periodic\n",
    "        # portion (1 <= i <= k) of embedding.\n",
    "        self.w_periodic = self.add_weight(\n",
    "            name='w_periodic',\n",
    "            shape=(input_shape[-1], self.embed_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b_periodic = self.add_weight(\n",
    "            name='b_periodic',\n",
    "            shape=(input_shape[1], self.embed_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Embed input into linear and periodic feature components.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor with shape (batch_size, sequence_length, feature_size)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor with shape (batch_size, sequence_length, embed_dim + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Linear term (i = 0).\n",
    "        embed_linear = tf.tensordot(x, self.w_linear, axes=1) + self.b_linear\n",
    "\n",
    "        # Periodic terms (1 <= i <= k).\n",
    "        inner = tf.tensordot(x, self.w_periodic, axes=1) + self.b_periodic\n",
    "        embed_periodic = self.activation_func(inner)\n",
    "\n",
    "        # Return concatenated linear and periodic features.\n",
    "        return tf.concat([embed_linear, embed_periodic], axis=-1)\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Retreive custom layer configuration for future loading.\n",
    "\n",
    "        Returns:\n",
    "            dict: Configuration dictionary.\n",
    "        \"\"\"\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Update custom objects dictionary.\n",
    "keras.utils.get_custom_objects()['Time2Vec'] = Time2Vec\n",
    "\n",
    "\n",
    "\n",
    "# stock_feat = 5\n",
    "# seq_len = 128\n",
    "# embed_dim = 32\n",
    "# inp = keras.Input(shape=(seq_len, stock_feat))\n",
    "# logger.info(f\"{inp.shape=}\")\n",
    "# x = Time2Vec(embed_dim)(inp)\n",
    "# logger.info(f\"{x.shape=}\")\n",
    "# x = keras.layers.Concatenate(axis=-1)([inp, x])\n",
    "# logger.info(f\"{x.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layers\n",
    "\n",
    "Currently uses attention layers provided by TensorFlow. See https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Layer\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/transformer#encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "        d_k: int,\n",
    "        d_v: int,\n",
    "        n_heads: int,\n",
    "        d_ff: int,\n",
    "        dropout: float = 0.0,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"Transformer encoder layer.\n",
    "\n",
    "        Based on the original concept proposed by Vaswani et al., 2017 (https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "        Args:\n",
    "            d_k (int): Key dimension (also used for Query dimension).\n",
    "            d_v (int): Value dimension.\n",
    "            n_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimension of the feed forward sublayer.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_k = d_k # Query and Key have same dimension.\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads # Number of attention heads.\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, input_shape: tuple[tf.TensorShape,tf.TensorShape,tf.TensorShape]):\n",
    "\n",
    "        # First sublayer.\n",
    "        # Multi-head attention with add and norm.\n",
    "        self.attn_multi = keras.layers.MultiHeadAttention(\n",
    "            num_heads=self.n_heads,\n",
    "            key_dim=self.d_k,\n",
    "            value_dim=self.d_v,\n",
    "        )\n",
    "        self.attn_multi._build_from_signature(*input_shape)\n",
    "        self.attn_dropout = keras.layers.Dropout(rate=self.dropout)\n",
    "        self.attn_add = keras.layers.Add()\n",
    "        self.attn_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Second sublayer.\n",
    "        # Feed forward with add and norm.\n",
    "        d_query_feat = input_shape[0][-1] # Query feature size.\n",
    "        self.ff_dense_1 = keras.layers.Dense(units=self.d_ff, activation='relu')\n",
    "        self.ff_dense_2 = keras.layers.Dense(units=d_query_feat)\n",
    "        self.ff_dropout = keras.layers.Dropout(rate=self.dropout)\n",
    "        self.ff_add = keras.layers.Add()\n",
    "        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x: tuple[tf.Tensor,tf.Tensor,tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"Encode input using multi-head self-attention mechanisms.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Tuple of Query, Value, and Key tensors. Note that the Key tensor is optional, if omitted the Value tensor will be used for both Key and Value.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor with shape (batch_size, sequence_length, embed_dim + 1)\n",
    "        \"\"\"\n",
    "        # x = (query, value, key)\n",
    "        # note that \"key\" is optional.\n",
    "\n",
    "        # First, do the attention sublayer.\n",
    "        x_attn = self.attn_multi(*x) # Unpack input as Query, Value, and optional Key.\n",
    "        x_attn = self.attn_dropout(x_attn)\n",
    "        x_attn = self.attn_add([x[0], x_attn]) # (residual) Add Query matrix with result of attention layer.\n",
    "        x_attn = self.attn_norm(x_attn) # Normalize the residual.\n",
    "\n",
    "        # Second, do the feed forward sublayer.\n",
    "        x_ff = self.ff_dense_1(x_attn)\n",
    "        x_ff = self.ff_dense_2(x_ff)\n",
    "        x_ff = self.ff_dropout(x_ff)\n",
    "        x_ff = self.ff_add([x_attn, x_ff])\n",
    "        x_ff = self.ff_norm(x_ff)\n",
    "\n",
    "        # Return output of feed forward sublayer.\n",
    "        return x_ff\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Retreive custom layer configuration for future loading.\n",
    "\n",
    "        Returns:\n",
    "            dict: Configuration dictionary.\n",
    "        \"\"\"\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_heads': self.n_heads,\n",
    "            'd_k': self.d_k,\n",
    "            'd_v': self.d_v,\n",
    "            'd_ff': self.d_ff,\n",
    "            'dropout': self.dropout,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Update custom objects dictionary.\n",
    "keras.utils.get_custom_objects()['TransformerEncoderLayer'] = TransformerEncoderLayer\n",
    "\n",
    "\n",
    "\n",
    "# stock_feat = 5\n",
    "# seq_len = 128\n",
    "# embed_dim = 32\n",
    "# d_k = 512\n",
    "# d_v = 256\n",
    "# n_heads = 8\n",
    "# d_ff = 512\n",
    "# inp = keras.Input(shape=(seq_len, stock_feat))\n",
    "# logger.info(f\"{inp.shape=}\")\n",
    "# x = Time2Vec(embed_dim)(inp)\n",
    "# logger.info(f\"Time2Vec {x.shape=}\")\n",
    "# x = keras.layers.Concatenate(axis=-1)([inp, x])\n",
    "# logger.info(f\"Concatenate {x.shape=}\")\n",
    "# x = TransformerEncoderLayer(d_k, d_v, n_heads, d_ff)([x, x, x])\n",
    "# logger.info(f\"TransformerEncoder {x.shape=}\")\n",
    "# x = TransformerEncoderLayer(d_k, d_v, n_heads, d_ff)([x, x, x])\n",
    "# logger.info(f\"TransformerEncoderLayer {x.shape=}\")\n",
    "# x = TransformerEncoderLayer(d_k, d_v, n_heads, d_ff)([x, x, x])\n",
    "# logger.info(f\"TransformerEncoderLayer {x.shape=}\")\n",
    "# x = keras.layers.GlobalAvgPool1D(data_format='channels_first')(x)\n",
    "# logger.info(f\"GlobalAvgPool1D {x.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(\n",
    "    in_seq_len: int,\n",
    "    in_feat: int,\n",
    "    out_feat: int,\n",
    "    fc_units: list[int], # list of fully-connected dimensions before classifier.\n",
    "    embed_dim: int,\n",
    "    d_k: int,\n",
    "    d_v: int,\n",
    "    n_heads: int,\n",
    "    d_ff: int,\n",
    "    dropout: float = 0.0,\n",
    "    n_encoders: int = 3,\n",
    "    ):\n",
    "\n",
    "    # Input sequence of features.\n",
    "    inp = keras.Input(shape=(in_seq_len, in_feat))\n",
    "    # Time embedding.\n",
    "    x = Time2Vec(embed_dim)(inp)\n",
    "    # Combine input with embedding to form attention input features.\n",
    "    x = keras.layers.Concatenate(axis=-1)([inp, x])\n",
    "    # Pass combined featured through cascaded self-attention encoder sublayers.\n",
    "    for _ in range(n_encoders):\n",
    "        x = TransformerEncoderLayer(\n",
    "            d_k=d_k,\n",
    "            d_v=d_v,\n",
    "            n_heads=n_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout,\n",
    "        )((x, x, x)) # (query, value, key)\n",
    "    # Downsample to the original sequence dimension.\n",
    "    x = keras.layers.GlobalAvgPool1D(data_format='channels_first')(x) # shape=(in_seq_len,)\n",
    "    x = keras.layers.Dropout(rate=dropout)(x)\n",
    "    # Fully-connected network before classifier.\n",
    "    for units in fc_units: \n",
    "        x = keras.layers.Dense(units=units, activation='relu')(x)\n",
    "        x = keras.layers.Dropout(rate=dropout)(x)\n",
    "    # Classifier.\n",
    "    x = keras.layers.Dense(units=out_feat, activation='linear')(x)\n",
    "\n",
    "    # Construct model class and return.\n",
    "    return keras.Model(inputs=inp, outputs=x)\n",
    "\n",
    "\n",
    "# kwargs = dict(\n",
    "#     in_seq_len=128, # Number of days in the past.\n",
    "#     in_feat=5, # Number of features for each day in the past.\n",
    "#     out_feat=3, # Number of features on 1-day horizon.\n",
    "#     fc_units=[64,64],\n",
    "#     embed_dim=32,\n",
    "#     d_k=512,\n",
    "#     d_v=256,\n",
    "#     n_heads=8,\n",
    "#     d_ff=512,\n",
    "# )\n",
    "# model = build_transformer(**kwargs)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_from_hparams(func):\n",
    "    \"\"\"Generalized model build and compile from hyperparameters.\"\"\"\n",
    "    def wrapper(hparams: dict, compile_params: dict):\n",
    "        \"\"\"Builds model using given hyperparameters for both model and optimizer, and compile parameters for compilation.\"\"\"\n",
    "        # Extract paramters needed for the model.\n",
    "        model_params = {k: hparams[k] for k in inspect.signature(func).parameters if k in hparams}\n",
    "        # Build model.\n",
    "        model = func(**model_params)\n",
    "        # Configure optimizer.\n",
    "        optim = keras.optimizers.get({'class_name': hparams['optim'], 'config': {'lr':hparams['lr']}})\n",
    "        # Compile the model.\n",
    "        model.compile(optimizer=optim, loss=compile_params['loss'], metrics=compile_params['metrics'])\n",
    "        return model\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "# builder = build_model_from_hparams(build_transformer)\n",
    "# hparams = dict(\n",
    "#     in_seq_len=128, # Number of days in the past.\n",
    "#     in_feat=5, # Number of features for each day in the past.\n",
    "#     out_feat=3, # Number of features on 1-day horizon.\n",
    "#     fc_units=[64,64],\n",
    "#     embed_dim=32,\n",
    "#     d_k=512,\n",
    "#     d_v=256,\n",
    "#     n_heads=8,\n",
    "#     d_ff=512,\n",
    "#     optim='adam',\n",
    "#     lr=0.001,\n",
    "# )\n",
    "# compile_params = dict(\n",
    "#     loss='mse',\n",
    "#     metrics=['mae', 'mape'],\n",
    "# )\n",
    "# model = builder(hparams, compile_params)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(metrics_path: str):\n",
    "    \"\"\"Load model metrics from file.\"\"\"\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_history(history_path: str):\n",
    "    \"\"\"Load model history from file.\"\"\"\n",
    "    return pd.read_csv(history_path)\n",
    "\n",
    "def load_trained_model(\n",
    "    checkpoint_path: str,\n",
    "    ) -> tuple[keras.models.Model]:\n",
    "    \"\"\"Helper to load a saved model.\"\"\"\n",
    "    model = keras.models.load_model(\n",
    "        checkpoint_path, \n",
    "        custom_objects=keras.utils.get_custom_objects(),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(\n",
    "    model,\n",
    "    model_name: str,\n",
    "    datagen_train: tf.data.Dataset,\n",
    "    datagen_val: tf.data.Dataset,\n",
    "    datagen_test: tf.data.Dataset,\n",
    "    epochs: int,\n",
    "    checkpoint_path: str,\n",
    "    history_path: str = None,\n",
    "    metrics_path: str = None,\n",
    "    ) -> tuple[keras.models.Model, dict, dict]:\n",
    "    \"\"\"Trains and evaluates a given model on the given datasets.\n",
    "\n",
    "    Args:\n",
    "        model (_type_): The model to train and evaluation.\n",
    "        model_name (str): String identifier for the model (used when saving some log files).\n",
    "        datagen_train (tf.data.Dataset): Training dataset.\n",
    "        datagen_val (tf.data.Dataset): Validation dataset.\n",
    "        datagen_test (tf.data.Dataset): Testing dataset.\n",
    "        epochs (int): Number of training epochs.\n",
    "        checkpoint_path (str): Path to checkpoint file\n",
    "        history_path (str, optional): Path to history CSV file. If None is provided, then the file will be located at the same path as the checkpoint file with name `\"history.csv\"`. Defaults to None.\n",
    "        metrics_path (str, optional): Path to metrics JSON file. If None is provided, then the file will be located at the same path as the checkpoint file with name `\"metrics.json\"`. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple[keras.models.Model, dict, dict: Tuple of trained model, history dictionary, and metrics dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure checkpoint root directory has been created.\n",
    "    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if history_path is None:\n",
    "        history_path = checkpoint_path.parent/'history.csv'\n",
    "    if metrics_path is None:\n",
    "        metrics_path = checkpoint_path.parent/'metrics.json'\n",
    "\n",
    "    # List of callbacks during training.\n",
    "    callbacks = [\n",
    "        # Save model checkpoint after every epoch.\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            monitor='val_acc',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        # Log training history to CSV file.\n",
    "        keras.callbacks.CSVLogger(\n",
    "            filename=history_path,\n",
    "            append=False,\n",
    "        ),\n",
    "        # Early stopping when performance does not improve across N epochs.\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_acc',\n",
    "            mode='max',\n",
    "            patience=2,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Train the model.\n",
    "    logger.info(f\"{model_name} fit\")\n",
    "    history = model.fit(datagen_train,\n",
    "        validation_data=datagen_val,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Evaluate the newly trained model.\n",
    "    test_loss, test_acc = model.evaluate(datagen_test)\n",
    "\n",
    "    # Create dictionary of metrics to return and preserve in file.\n",
    "    metrics = {\n",
    "        'acc': history.history['acc'][-1],\n",
    "        'val_acc': history.history['val_acc'][-1],\n",
    "        'test_acc': test_acc,\n",
    "        'loss': history.history['loss'][-1],\n",
    "        'val_loss': history.history['val_loss'][-1],\n",
    "        'test_loss': test_loss,\n",
    "    }\n",
    "\n",
    "    # Dump metrics to JSON file.\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    return model, history.history, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(history: dict, model_name: str) -> plt.Figure:\n",
    "    \"\"\"Plots model training and validation accuracy.\"\"\"\n",
    "    fig = plt.figure()\n",
    "    plt.plot(history['acc'], label='train')\n",
    "    plt.plot(history['val_acc'], label='val')\n",
    "    plt.xlim(0, len(history['acc'])-1)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f\"{model_name} Accuracy\")\n",
    "    plt.legend(loc='upper left')\n",
    "    return fig\n",
    "\n",
    "def plot_loss(history: dict, model_name: str) -> plt.Figure:\n",
    "    \"\"\"Plots model training and validation loss.\"\"\"\n",
    "    fig = plt.figure()\n",
    "    plt.plot(history['loss'], label='train')\n",
    "    plt.plot(history['val_loss'], label='val')\n",
    "    plt.xlim(0, len(history['loss'])-1)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f\"{model_name} Loss\")\n",
    "    plt.legend(loc='upper left')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_for_dataset(\n",
    "    model_name: str,\n",
    "    build_model_func,\n",
    "    hparams: dict,\n",
    "    compile_params: dict,\n",
    "    asset: str,\n",
    "    in_seq_len: int,\n",
    "    out_seq_len: int,\n",
    "    shift: int,\n",
    "    split: tuple[float, float, float] = (0.7, 0.2, 0.1),\n",
    "    in_feat: list[str] = ['Open','High','Low','Close','Volume'],\n",
    "    out_feat: list[str] = ['Open','High','Low','Close','Volume'],\n",
    "    batch_size: int = 64,\n",
    "    shuffle: bool = True,\n",
    "    strategy: tf.distribute.Strategy = tf.distribute.get_strategy(),\n",
    "    epochs: int = 10,\n",
    "    ) -> tuple[keras.models.Model, dict, dict, dict, pd.DataFrame]:\n",
    "    \"\"\"Train and evaluate a model on a given dataset.\n",
    "\n",
    "    If checkpoint exists then the model is loaded in place of training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Train and evaluate model.\n",
    "    checkpoint_path = CHECKPOINT_ROOT/model_name/'model.h5'\n",
    "    history_path = checkpoint_path.parent/'history.csv'\n",
    "    metrics_path = checkpoint_path.parent/'metrics.json'\n",
    "\n",
    "    # Load model from best checkpoint.\n",
    "    if checkpoint_path.exists() or history_path.exists() or metrics_path.exists():\n",
    "        # Load the model.\n",
    "        logger.info(f\"[{model_name}] Loading best model from: {checkpoint_path}\")\n",
    "        model = keras.models.load_model(checkpoint_path, custom_objects=keras.utils.get_custom_objects())\n",
    "\n",
    "        # Load history and metrics.\n",
    "        logger.info(f\"[{model_name}] Loading from save data\")\n",
    "        hist = load_history(history_path)\n",
    "        met = load_metrics(metrics_path)\n",
    "\n",
    "    # Train model.\n",
    "    else:\n",
    "\n",
    "        # Maximize batch size efficiency using distributed strategy.\n",
    "        batch_size_per_replica = batch_size\n",
    "        batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n",
    "\n",
    "        logger.info(f\"[{model_name}] Training new model: {epochs=}, {batch_size=}, {strategy=}\")\n",
    "\n",
    "        # Under strategy scope.\n",
    "        with strategy.scope():\n",
    "\n",
    "            # Load the dataset.\n",
    "            (dataset_train, dataset_val, dataset_test) = load_stock_dataset(\n",
    "                asset=asset,\n",
    "                in_seq_len=in_seq_len,\n",
    "                out_seq_len=out_seq_len,\n",
    "                shift=shift,\n",
    "                split=split,\n",
    "                in_feat=in_feat,\n",
    "                out_feat=out_feat,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "            )\n",
    "\n",
    "            # Create and compile model.\n",
    "            model = build_model_func(\n",
    "                hparams=dict(\n",
    "                    in_seq_len=in_seq_len,\n",
    "                    in_feat=len(in_feat), # Number of input features.\n",
    "                    out_feat=len(out_feat), # Number of output features.\n",
    "                    **hparams,\n",
    "                ),\n",
    "                compile_params=compile_params,\n",
    "            )\n",
    "\n",
    "            model.summary(print_fn=logger.info)\n",
    "\n",
    "        # Train the model using the strategy.\n",
    "        model, hist, met = train_evaluate_model(\n",
    "            model,\n",
    "            model_name=model_name,\n",
    "            datagen_train=dataset_train,\n",
    "            datagen_val=dataset_val,\n",
    "            datagen_test=dataset_test,\n",
    "            epochs=epochs,\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            history_path=history_path,\n",
    "            metrics_path=metrics_path,\n",
    "        )\n",
    "\n",
    "    # Print model summary.\n",
    "    logger.info(f\"[{model_name}] Model Summary:\")\n",
    "    model.summary(print_fn=logger.info)\n",
    "\n",
    "    logger.info(f\"[{model_name}] Training and Evaluation Results:\")\n",
    "\n",
    "    # Build dataframe using results.\n",
    "    df = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        **met,\n",
    "        **hparams,\n",
    "    }])\n",
    "    logger.info(df.to_string(index=False)) # Log to console.\n",
    "\n",
    "    # Plot the train/val performance.\n",
    "    fig = plot_acc(hist, model_name=model_name.upper())\n",
    "    fig.savefig(IMAGE_ROOT/f\"{model_name}_acc.png\", bbox_inches='tight')\n",
    "    fig.show()\n",
    "\n",
    "    # Plot the train/val loss.\n",
    "    fig = plot_loss(hist, model_name=model_name.upper())\n",
    "    fig.savefig(IMAGE_ROOT/f\"{model_name}_loss.png\", bbox_inches='tight')\n",
    "    fig.show()\n",
    "\n",
    "    return model, hist, met, hparams, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = build_model_from_hparams(build_transformer)\n",
    "\n",
    "hparams = dict(\n",
    "    fc_units=[64,64],\n",
    "    embed_dim=32,\n",
    "    d_k=512,\n",
    "    d_v=256,\n",
    "    n_heads=8,\n",
    "    d_ff=512,\n",
    "    optim='adam',\n",
    "    lr=0.001,\n",
    ")\n",
    "compile_params = dict(\n",
    "    loss='mse',\n",
    "    metrics=['mae', 'mape'],\n",
    ")\n",
    "\n",
    "model, hist, met, hparams, df = train_evaluate_for_dataset(\n",
    "    model_name='transformer',\n",
    "    build_model_func=builder,\n",
    "    hparams=hparams,\n",
    "    compile_params=compile_params,\n",
    "    asset='aapl',\n",
    "    in_seq_len=128,\n",
    "    out_seq_len=1,\n",
    "    shift=1,\n",
    "    split=(0.7, 0.2, 0.1),\n",
    "    in_feat=['Open','High','Low','Close','Volume'],\n",
    "    out_feat=['Open','High','Low','Close','Volume'],\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    strategy=tf.distribute.get_strategy(),\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e27c2da514be0f4555df3a4c15a4c6256ef40203ad64abea68e2343d203af1e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
