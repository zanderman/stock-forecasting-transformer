{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Forecasting using Transformers\n",
    "\n",
    "In this notebook we implement a Transformer model to forecast stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/a/64438413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "This section contains code that is modifies output path locations, random seed, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = Path('~/ml/datasets').expanduser()\n",
    "if not DATASET_ROOT.exists(): raise ValueError(f\"Dataset root directory does not exist at {DATASET_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds.\n",
    "SEED = 0\n",
    "tf.random.set_seed(SEED) # Only this works on ARC (since tensorflow==2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging (useful for ARC systems).\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) # Must be lowest of all handlers listed below.\n",
    "while logger.hasHandlers(): logger.removeHandler(logger.handlers[0]) # Clear all existing handlers.\n",
    "\n",
    "# Custom log formatting.\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "# Log to STDOUT (uses default formatting).\n",
    "sh = logging.StreamHandler(stream=sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "# Set Tensorflow logging level.\n",
    "tf.get_logger().setLevel('ERROR') # 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API configured\n"
     ]
    }
   ],
   "source": [
    "# Request user for Kaggle login if JSON file does not exist.\n",
    "kaggle_config_file = Path(\"~/.kaggle/kaggle.json\").expanduser()\n",
    "if not kaggle_config_file.exists() and os.environ.get(\"KAGGLE_USERNAME\", None) is None and os.environ.get(\"KAGGLE_KEY\", None) is None:\n",
    "    import json\n",
    "    import getpass\n",
    "    entry = getpass.getpass(prompt=\"Please enter your Kaggle username or JSON blob: \")\n",
    "    try:\n",
    "        blob = json.loads(entry)\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = blob['username']\n",
    "        os.environ[\"KAGGLE_KEY\"] = blob['key']\n",
    "    except:\n",
    "        api_key = getpass.getpass(prompt=\"Please enter your Kaggle API KEY: \")\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = entry\n",
    "        os.environ[\"KAGGLE_KEY\"] = api_key\n",
    "else:\n",
    "    logger.info('Kaggle API configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "# List all GPUs visible to TensorFlow.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "logger.info(f\"Num GPUs Available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    logger.info(f\"Name: {gpu.name}, Type: {gpu.device_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huge Stock Market Dataset from Kaggle\n",
    "\n",
    "https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HugeStockMarketDataset:\n",
    "    \"\"\"Wrapper for Huge Stock Market Dataset by Boris Marjanovic on Kaggle.\n",
    "\n",
    "    Source URL is: https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs/version/3\n",
    "\n",
    "    This class can be used like a Python dictionary, where keys are the stock/etf names, and values are\n",
    "    `pandas.DataFrame` objects corresponding to that stock/etf.\n",
    "    \"\"\"\n",
    "    root = 'HugeStockMarketDataset'\n",
    "\n",
    "    def __init__(self, \n",
    "        path: str,\n",
    "        files: list = None,\n",
    "        quiet: bool = False,\n",
    "        exclude_stocks: bool = False,\n",
    "        exclude_etfs: bool = False,\n",
    "        usecols: list[str] = ['Date','Open','High','Low','Close','Volume','OpenInt'],\n",
    "        ):\n",
    "        self.exclude_stocks = exclude_stocks\n",
    "        self.exclude_etfs = exclude_etfs\n",
    "        self.usecols = usecols\n",
    "        self._index = {}\n",
    "\n",
    "        # Download the dataset if necessary.\n",
    "        newpath = Path(path).expanduser()/self.root\n",
    "        if not newpath.exists():\n",
    "            self.download(newpath, files, quiet=quiet)\n",
    "        else:\n",
    "            self.path = newpath\n",
    "            self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"Creates an internal index of stocks and ETFs for lookup.\"\"\"\n",
    "\n",
    "        # Helper function to index a folder of files.\n",
    "        def _index_folder(dir: Path):\n",
    "            for file in glob.iglob(str(dir/'*.txt'), recursive=True):\n",
    "                filename = Path(file).name\n",
    "                product_name = filename.split('.', maxsplit=1)[0]\n",
    "                self._index[product_name] = file\n",
    "\n",
    "        # Index all stocks.\n",
    "        if not self.exclude_stocks:\n",
    "            _index_folder(self.path/'Stocks')\n",
    "        \n",
    "        # Index all ETFs.\n",
    "        if not self.exclude_etfs:\n",
    "            _index_folder(self.path/'ETFs')\n",
    "\n",
    "    def download(self, path: str, files: list = None, quiet: bool = True):\n",
    "        \"\"\"Downloads the dataset from Kaggle.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to place the download.\n",
    "            files (list, optional): Subset list of files to download instead of entire dataset. Defaults to None.\n",
    "            quiet (bool, optional): Suppress verbose output. Defaults to True.\n",
    "        \"\"\"\n",
    "        import kaggle\n",
    "        kaggle_dataset = 'borismarjanovic/price-volume-data-for-all-us-stocks-etfs'\n",
    "        kaggle.api.authenticate()\n",
    "\n",
    "        # Save the new downloaded path.\n",
    "        self.path = Path(path).expanduser()\n",
    "\n",
    "        # Specific file list was given.\n",
    "        if files is not None:\n",
    "            for f in files:\n",
    "                kaggle.api.dataset_download_file(\n",
    "                    dataset=kaggle_dataset,\n",
    "                    file_name=f,\n",
    "                    path=path/f,\n",
    "                    quiet=quiet,\n",
    "                )\n",
    "        # Download all files.\n",
    "        else:\n",
    "            kaggle.api.dataset_download_files(\n",
    "                dataset=kaggle_dataset,\n",
    "                path=path,\n",
    "                unzip=True,\n",
    "                quiet=quiet,\n",
    "            )\n",
    "\n",
    "        # Force rebuild the index after downloading.\n",
    "        logger.info(\"Building file index\")\n",
    "        self._build_index()\n",
    "\n",
    "    def get_dataframe(self, \n",
    "        key: str,\n",
    "        **kwargs,\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"Obtain historical data for stock or ETF in a pandas dataframe.\n",
    "\n",
    "        Optional keyword arguments are passed directly to `pandas.read_csv` function.\n",
    "\n",
    "        Args:\n",
    "            key (str): The identifier for the stock or ETF.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Historical data.\n",
    "        \"\"\"\n",
    "        return pd.read_csv(self._index[key], **kwargs)\n",
    "\n",
    "    #### Dictionary Override ######\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, str):\n",
    "            return self.get_dataframe(key)\n",
    "        elif isinstance(key, list):\n",
    "            return [self.get_dataframe(asset) for asset in key]\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        del self._index[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._index)\n",
    "\n",
    "    def items(self):\n",
    "        for key in self:\n",
    "            yield key, self.get_dataframe(key)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._index)\n",
    "\n",
    "    def keys(self):\n",
    "        \"\"\"Returns a list of all downloaded stocks and ETFs.\"\"\"\n",
    "        return self._index.keys()\n",
    "\n",
    "    ###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets actually load the dataset. In this case, we're only looking at the `aapl` stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HugeStockMarketDataset(DATASET_ROOT)\n",
    "df = dataset['aapl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time2Vec Embedding\n",
    "\n",
    "https://arxiv.org/abs/1907.05321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp.shape=TensorShape([None, 128, 5])\n",
      "x.shape=TensorShape([None, 128, 33])\n",
      "x.shape=TensorShape([None, 128, 38])\n"
     ]
    }
   ],
   "source": [
    "class Time2Vec(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim: int, activation: str = 'sin', **kwargs):\n",
    "        \"\"\"Vector embedding representation of time.\n",
    "\n",
    "        Based on the original concept proposed by Kazemi et al., 2019 (https://arxiv.org/abs/1907.05321).\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Length of the time embedding vector.\n",
    "            activation (str, optional): Periodic activation function. Possible values are ['sin', 'cos']. Defaults to 'sin'.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.activation = activation.lower() # Convert to lower-case.\n",
    "\n",
    "        # Set periodic activation function.\n",
    "        if self.activation.startswith('sin'):\n",
    "            self.activation_func = tf.sin\n",
    "        elif self.activation.startswith('cos'):\n",
    "            self.activation_func = tf.cos\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported periodic activation function \"{activation}\"')\n",
    "\n",
    "    def build(self, input_shape: list[int]):\n",
    "\n",
    "        # Weight and bias term for linear portion (i = 0)\n",
    "        # of embedding.\n",
    "        self.w_linear = self.add_weight(\n",
    "            name='w_linear',\n",
    "            shape=(input_shape[1], 1,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b_linear = self.add_weight(\n",
    "            name='b_linear',\n",
    "            shape=(input_shape[1], 1,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Weight and bias terms for the periodic\n",
    "        # portion (1 <= i <= k) of embedding.\n",
    "        self.w_periodic = self.add_weight(\n",
    "            name='w_periodic',\n",
    "            shape=(input_shape[-1], self.embed_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b_periodic = self.add_weight(\n",
    "            name='b_periodic',\n",
    "            shape=(input_shape[1], self.embed_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Embed input into linear and periodic feature components.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor with shape (batch_size, sequence_length, feature_size)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor with shape (batch_size, sequence_length, embed_dim + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Linear term (i = 0).\n",
    "        embed_linear = tf.tensordot(x, self.w_linear, axes=1) + self.b_linear\n",
    "\n",
    "        # Periodic terms (1 <= i <= k).\n",
    "        inner = tf.tensordot(x, self.w_periodic, axes=1) + self.b_periodic\n",
    "        embed_periodic = self.activation_func(inner)\n",
    "\n",
    "        # Return concatenated linear and periodic features.\n",
    "        return tf.concat([embed_linear, embed_periodic], axis=-1)\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Retreive custom layer configuration for future loading.\n",
    "\n",
    "        Returns:\n",
    "            dict: Configuration dictionary.\n",
    "        \"\"\"\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "stock_feat = 5\n",
    "seq_len = 128\n",
    "embed_dim = 32\n",
    "inp = keras.Input(shape=(seq_len, stock_feat))\n",
    "logger.info(f\"{inp.shape=}\")\n",
    "x = Time2Vec(embed_dim)(inp)\n",
    "logger.info(f\"{x.shape=}\")\n",
    "x = keras.layers.Concatenate(axis=-1)([inp, x])\n",
    "logger.info(f\"{x.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layers\n",
    "\n",
    "Currently uses attention layers provided by TensorFlow. See https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(keras.layers.Layer):\n",
    "#     def __init__(self, d_k: int, d_v: int, n_heads: int):\n",
    "#         \"\"\"Single-head attention layer.\n",
    "\n",
    "#         Based on the original concept proposed by Vaswani et al., 2017 (https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "#         Args:\n",
    "#             d_k (int): Key dimension (also used for Query dimension).\n",
    "#             d_v (int): Value dimension.\n",
    "#             n_heads (int): Number of attention heads.\n",
    "#         \"\"\"\n",
    "#         self.d_k = d_k # Query and Key have same dimension.\n",
    "#         self.d_v = d_v\n",
    "#         self.n_heads = n_heads # Number of attention heads.\n",
    "#         self.heads = [] # List of attention layers as heads.\n",
    "\n",
    "#     def build(self, input_shape: list[int]):\n",
    "\n",
    "#         # Build attention heads.\n",
    "#         self.heads = [\n",
    "#             keras.layers.Attention()\n",
    "#             for i in range(self.n_heads)\n",
    "#         ]\n",
    "\n",
    "#         # Build linear relationship between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(keras.layers.Layer):\n",
    "#     def __init__(self, d_k: int, d_v: int):\n",
    "#         \"\"\"Single-head attention layer.\n",
    "\n",
    "#         Based on the original concept proposed by Vaswani et al., 2017 (https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "#         Args:\n",
    "#             d_k (int): Key dimension (also used for Query dimension).\n",
    "#             d_v (int): Value dimension.\n",
    "#         \"\"\"\n",
    "#         self.d_k = d_k # Query and Key have same dimension.\n",
    "#         self.d_v = d_v\n",
    "\n",
    "#     def build(self, input_shape: list[int]):\n",
    "#         self.query = keras.layers.Dense(\n",
    "#             units=self.d_k,\n",
    "#             input_shape=input_shape,\n",
    "#             kernel_initializer='glorot_uniform', \n",
    "#             bias_initializer='glorot_uniform',\n",
    "#         )\n",
    "#         self.key = keras.layers.Dense(\n",
    "#             units=self.d_k,\n",
    "#             input_shape=input_shape,\n",
    "#             kernel_initializer='glorot_uniform', \n",
    "#             bias_initializer='glorot_uniform',\n",
    "#         )\n",
    "#         self.value = keras.layers.Dense(\n",
    "#             units=self.d_v,\n",
    "#             input_shape=input_shape,\n",
    "#             kernel_initializer='glorot_uniform', \n",
    "#             bias_initializer='glorot_uniform',\n",
    "#         )\n",
    "\n",
    "#     def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        \n",
    "\n",
    "\n",
    "#     def get_config(self) -> dict:\n",
    "#         \"\"\"Retreive custom layer configuration for future loading.\n",
    "\n",
    "#         Returns:\n",
    "#             dict: Configuration dictionary.\n",
    "#         \"\"\"\n",
    "#         config = super().get_config().copy()\n",
    "#         config.update({\n",
    "#             'd_k': self.d_k,\n",
    "#             'd_v': self.d_v,\n",
    "#         })\n",
    "#         return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp.shape=TensorShape([None, 128, 5])\n",
      "Time2Vec x.shape=TensorShape([None, 128, 33])\n",
      "Concatenate x.shape=TensorShape([None, 128, 38])\n",
      "TransformerEncoder x.shape=TensorShape([None, 128, 38])\n",
      "TransformerEncoder x.shape=TensorShape([None, 128, 38])\n",
      "TransformerEncoder x.shape=TensorShape([None, 128, 38])\n",
      "GlobalAvgPool1D x.shape=TensorShape([None, 128])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "        d_k: int,\n",
    "        d_v: int,\n",
    "        n_heads: int,\n",
    "        d_model: int,\n",
    "        dropout: float = 0.0,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"Transformer encoder layer.\n",
    "\n",
    "        Based on the original concept proposed by Vaswani et al., 2017 (https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "        Args:\n",
    "            d_k (int): Key dimension (also used for Query dimension).\n",
    "            d_v (int): Value dimension.\n",
    "            n_heads (int): Number of attention heads.\n",
    "            d_model (int): Dimension of the feed forward sublayer.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_k = d_k # Query and Key have same dimension.\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads # Number of attention heads.\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, input_shape: tuple[tf.TensorShape,tf.TensorShape,tf.TensorShape]):\n",
    "\n",
    "        # First sublayer.\n",
    "        # Multi-head attention with add and norm.\n",
    "        self.attn_multi = keras.layers.MultiHeadAttention(\n",
    "            num_heads=self.n_heads,\n",
    "            key_dim=self.d_k,\n",
    "            value_dim=self.d_v,\n",
    "        )\n",
    "        self.attn_multi._build_from_signature(*input_shape)\n",
    "        self.attn_dropout = keras.layers.Dropout(rate=self.dropout)\n",
    "        self.attn_add = keras.layers.Add()\n",
    "        self.attn_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Second sublayer.\n",
    "        # Feed forward with add and norm.\n",
    "        d_query_feat = input_shape[0][-1] # Query feature size.\n",
    "        self.ff_dense_1 = keras.layers.Dense(units=self.d_model, activation='relu')\n",
    "        self.ff_dense_2 = keras.layers.Dense(units=d_query_feat)\n",
    "        self.ff_dropout = keras.layers.Dropout(rate=self.dropout)\n",
    "        self.ff_add = keras.layers.Add()\n",
    "        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x: tuple[tf.Tensor,tf.Tensor,tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"Encode input using multi-head self-attention mechanisms.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Tuple of Query, Value, and Key tensors. Note that the Key tensor is optional, if omitted the Value tensor will be used for both Key and Value.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor with shape (batch_size, sequence_length, embed_dim + 1)\n",
    "        \"\"\"\n",
    "        # x = (query, value, key)\n",
    "        # note that \"key\" is optional.\n",
    "\n",
    "        # First, do the attention sublayer.\n",
    "        x_attn = self.attn_multi(*x) # Unpack input as Query, Value, and optional Key.\n",
    "        x_attn = self.attn_dropout(x_attn)\n",
    "        x_attn = self.attn_add([x[0], x_attn]) # (residual) Add Query matrix with result of attention layer.\n",
    "        x_attn = self.attn_norm(x_attn) # Normalize the residual.\n",
    "\n",
    "        # Second, do the feed forward sublayer.\n",
    "        x_ff = self.ff_dense_1(x_attn)\n",
    "        x_ff = self.ff_dense_2(x_ff)\n",
    "        x_ff = self.ff_dropout(x_ff)\n",
    "        x_ff = self.ff_add([x_attn, x_ff])\n",
    "        x_ff = self.ff_norm(x_ff)\n",
    "\n",
    "        # Return output of feed forward sublayer.\n",
    "        return x_ff\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Retreive custom layer configuration for future loading.\n",
    "\n",
    "        Returns:\n",
    "            dict: Configuration dictionary.\n",
    "        \"\"\"\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_heads': self.n_heads,\n",
    "            'd_k': self.d_k,\n",
    "            'd_v': self.d_v,\n",
    "            'd_model': self.d_model,\n",
    "            'dropout': self.dropout,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "stock_feat = 5\n",
    "seq_len = 128\n",
    "embed_dim = 32\n",
    "d_k = 512\n",
    "d_v = 256\n",
    "n_heads = 8\n",
    "d_model = 512\n",
    "inp = keras.Input(shape=(seq_len, stock_feat))\n",
    "logger.info(f\"{inp.shape=}\")\n",
    "x = Time2Vec(embed_dim)(inp)\n",
    "logger.info(f\"Time2Vec {x.shape=}\")\n",
    "x = keras.layers.Concatenate(axis=-1)([inp, x])\n",
    "logger.info(f\"Concatenate {x.shape=}\")\n",
    "x = TransformerEncoder(d_k, d_v, n_heads, d_model)([x, x, x])\n",
    "logger.info(f\"TransformerEncoder {x.shape=}\")\n",
    "x = TransformerEncoder(d_k, d_v, n_heads, d_model)([x, x, x])\n",
    "logger.info(f\"TransformerEncoder {x.shape=}\")\n",
    "x = TransformerEncoder(d_k, d_v, n_heads, d_model)([x, x, x])\n",
    "logger.info(f\"TransformerEncoder {x.shape=}\")\n",
    "x = keras.layers.GlobalAvgPool1D(data_format='channels_first')(x)\n",
    "logger.info(f\"GlobalAvgPool1D {x.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 128, 5)]     0           []                               \n",
      "                                                                                                  \n",
      " time2_vec_2 (Time2Vec)         (None, 128, 33)      4512        ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 38)      0           ['input_4[0][0]',                \n",
      "                                                                  'time2_vec_2[0][0]']            \n",
      "                                                                                                  \n",
      " transformer_encoder_3 (Transfo  (None, 128, 38)     516836      ['concatenate_2[0][0]',          \n",
      " rmerEncoder)                                                     'concatenate_2[0][0]',          \n",
      "                                                                  'concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " transformer_encoder_4 (Transfo  (None, 128, 38)     516836      ['transformer_encoder_3[0][0]',  \n",
      " rmerEncoder)                                                     'transformer_encoder_3[0][0]',  \n",
      "                                                                  'transformer_encoder_3[0][0]']  \n",
      "                                                                                                  \n",
      " transformer_encoder_5 (Transfo  (None, 128, 38)     516836      ['transformer_encoder_4[0][0]',  \n",
      " rmerEncoder)                                                     'transformer_encoder_4[0][0]',  \n",
      "                                                                  'transformer_encoder_4[0][0]']  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 128)         0           ['transformer_encoder_5[0][0]']  \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           4160        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3)            195         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,567,631\n",
      "Trainable params: 1,567,631\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(\n",
    "    in_seq_len: int,\n",
    "    in_feat: int,\n",
    "    out_feat: int,\n",
    "    fc_units: list[int], # list of fully-connected dimensions before classifier.\n",
    "    embed_dim: int,\n",
    "    d_k: int,\n",
    "    d_v: int,\n",
    "    n_heads: int,\n",
    "    d_model: int,\n",
    "    dropout: float = 0.0,\n",
    "    n_encoders: int = 3,\n",
    "    ):\n",
    "\n",
    "    # Input sequence of features.\n",
    "    inp = keras.Input(shape=(in_seq_len, in_feat))\n",
    "    # Time embedding.\n",
    "    x = Time2Vec(embed_dim)(inp)\n",
    "    # Combine input with embedding to form attention input features.\n",
    "    x = keras.layers.Concatenate(axis=-1)([inp, x])\n",
    "    # Pass combined featured through cascaded self-attention encoder sublayers.\n",
    "    for _ in range(n_encoders):\n",
    "        x = TransformerEncoder(\n",
    "            d_k=d_k,\n",
    "            d_v=d_v,\n",
    "            n_heads=n_heads,\n",
    "            d_model=d_model,\n",
    "            dropout=dropout,\n",
    "        )((x, x, x)) # (query, value, key)\n",
    "    # Downsample to the original sequence dimension.\n",
    "    x = keras.layers.GlobalAvgPool1D(data_format='channels_first')(x) # shape=(in_seq_len,)\n",
    "    x = keras.layers.Dropout(rate=dropout)(x)\n",
    "    # Fully-connected network before classifier.\n",
    "    for units in fc_units: \n",
    "        x = keras.layers.Dense(units=units, activation='relu')(x)\n",
    "        x = keras.layers.Dropout(rate=dropout)(x)\n",
    "    # Classifier.\n",
    "    x = keras.layers.Dense(units=out_feat, activation='linear')(x)\n",
    "\n",
    "    # Construct model class and return.\n",
    "    return keras.Model(inputs=inp, outputs=x)\n",
    "\n",
    "\n",
    "kwargs = dict(\n",
    "    in_seq_len=128, # Number of days in the past.\n",
    "    in_feat=5, # Number of features for each day in the past.\n",
    "    out_feat=3, # Number of features on 1-day horizon.\n",
    "    fc_units=[64,64],\n",
    "    embed_dim=32,\n",
    "    d_k=512,\n",
    "    d_v=256,\n",
    "    n_heads=8,\n",
    "    d_model=512,\n",
    ")\n",
    "model = build_model(**kwargs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e27c2da514be0f4555df3a4c15a4c6256ef40203ad64abea68e2343d203af1e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
