{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Forecasting using Transformers\n",
    "\n",
    "In this notebook we implement a Transformer model to forecast stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/a/64438413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time2Vec Embedding\n",
    "\n",
    "https://arxiv.org/abs/1907.05321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp.shape=TensorShape([None, 128, 5])\n",
      "x.shape=TensorShape([None, 128, 33])\n",
      "x.shape=TensorShape([None, 128, 38])\n"
     ]
    }
   ],
   "source": [
    "class Time2Vec(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim: int, activation: str = 'sin', **kwargs):\n",
    "        \"\"\"Vector embedding representation of time.\n",
    "\n",
    "        Based on the original concept proposed by Kazemi et al., 2019 (https://arxiv.org/abs/1907.05321).\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Length of the time embedding vector.\n",
    "            activation (str, optional): Periodic activation function. Possible values are ['sin', 'cos']. Defaults to 'sin'.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.activation = activation.lower() # Convert to lower-case.\n",
    "\n",
    "        # Set periodic activation function.\n",
    "        if self.activation.startswith('sin'):\n",
    "            self.activation_func = tf.sin\n",
    "        elif self.activation.startswith('cos'):\n",
    "            self.activation_func = tf.cos\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported periodic activation function \"{activation}\"')\n",
    "\n",
    "    def build(self, input_shape: list[int]):\n",
    "\n",
    "        # Weight and bias term for linear portion (i = 0)\n",
    "        # of embedding.\n",
    "        self.w_linear = self.add_weight(\n",
    "            name='w_linear',\n",
    "            shape=(input_shape[1], 1,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b_linear = self.add_weight(\n",
    "            name='b_linear',\n",
    "            shape=(input_shape[1], 1,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Weight and bias terms for the periodic\n",
    "        # portion (1 <= i <= k) of embedding.\n",
    "        self.w_periodic = self.add_weight(\n",
    "            name='w_periodic',\n",
    "            shape=(input_shape[-1], self.embed_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b_periodic = self.add_weight(\n",
    "            name='b_periodic',\n",
    "            shape=(input_shape[1], self.embed_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Embed input into linear and periodic feature components.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor with shape (batch_size, sequence_length, feature_size)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor with shape (batch_size, sequence_length, embed_dim + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Linear term (i = 0).\n",
    "        embed_linear = tf.tensordot(x, self.w_linear, axes=1) + self.b_linear\n",
    "\n",
    "        # Periodic terms (1 <= i <= k).\n",
    "        inner = tf.tensordot(x, self.w_periodic, axes=1) + self.b_periodic\n",
    "        embed_periodic = self.activation_func(inner)\n",
    "\n",
    "        # Return concatenated linear and periodic features.\n",
    "        return tf.concat([embed_linear, embed_periodic], axis=-1)\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Retreive custom layer configuration for future loading.\n",
    "\n",
    "        Returns:\n",
    "            dict: Configuration dictionary.\n",
    "        \"\"\"\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "stock_feat = 5\n",
    "seq_len = 128\n",
    "embed_dim = 32\n",
    "inp = keras.Input(shape=(seq_len, stock_feat))\n",
    "print(f\"{inp.shape=}\")\n",
    "x = Time2Vec(embed_dim)(inp)\n",
    "print(f\"{x.shape=}\")\n",
    "x = keras.layers.Concatenate(axis=-1)([inp, x])\n",
    "print(f\"{x.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layers\n",
    "\n",
    "Currently uses attention layers provided by TensorFlow. See https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(keras.layers.Layer):\n",
    "#     def __init__(self, d_k: int, d_v: int, n_heads: int):\n",
    "#         \"\"\"Single-head attention layer.\n",
    "\n",
    "#         Based on the original concept proposed by Vaswani et al., 2017 (https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "#         Args:\n",
    "#             d_k (int): Key dimension (also used for Query dimension).\n",
    "#             d_v (int): Value dimension.\n",
    "#             n_heads (int): Number of attention heads.\n",
    "#         \"\"\"\n",
    "#         self.d_k = d_k # Query and Key have same dimension.\n",
    "#         self.d_v = d_v\n",
    "#         self.n_heads = n_heads # Number of attention heads.\n",
    "#         self.heads = [] # List of attention layers as heads.\n",
    "\n",
    "#     def build(self, input_shape: list[int]):\n",
    "\n",
    "#         # Build attention heads.\n",
    "#         self.heads = [\n",
    "#             keras.layers.Attention()\n",
    "#             for i in range(self.n_heads)\n",
    "#         ]\n",
    "\n",
    "#         # Build linear relationship between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(keras.layers.Layer):\n",
    "#     def __init__(self, d_k: int, d_v: int):\n",
    "#         \"\"\"Single-head attention layer.\n",
    "\n",
    "#         Based on the original concept proposed by Vaswani et al., 2017 (https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "#         Args:\n",
    "#             d_k (int): Key dimension (also used for Query dimension).\n",
    "#             d_v (int): Value dimension.\n",
    "#         \"\"\"\n",
    "#         self.d_k = d_k # Query and Key have same dimension.\n",
    "#         self.d_v = d_v\n",
    "\n",
    "#     def build(self, input_shape: list[int]):\n",
    "#         self.query = keras.layers.Dense(\n",
    "#             units=self.d_k,\n",
    "#             input_shape=input_shape,\n",
    "#             kernel_initializer='glorot_uniform', \n",
    "#             bias_initializer='glorot_uniform',\n",
    "#         )\n",
    "#         self.key = keras.layers.Dense(\n",
    "#             units=self.d_k,\n",
    "#             input_shape=input_shape,\n",
    "#             kernel_initializer='glorot_uniform', \n",
    "#             bias_initializer='glorot_uniform',\n",
    "#         )\n",
    "#         self.value = keras.layers.Dense(\n",
    "#             units=self.d_v,\n",
    "#             input_shape=input_shape,\n",
    "#             kernel_initializer='glorot_uniform', \n",
    "#             bias_initializer='glorot_uniform',\n",
    "#         )\n",
    "\n",
    "#     def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        \n",
    "\n",
    "\n",
    "#     def get_config(self) -> dict:\n",
    "#         \"\"\"Retreive custom layer configuration for future loading.\n",
    "\n",
    "#         Returns:\n",
    "#             dict: Configuration dictionary.\n",
    "#         \"\"\"\n",
    "#         config = super().get_config().copy()\n",
    "#         config.update({\n",
    "#             'd_k': self.d_k,\n",
    "#             'd_v': self.d_v,\n",
    "#         })\n",
    "#         return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp.shape=TensorShape([None, 128, 5])\n",
      "Time2Vec x.shape=TensorShape([None, 128, 33])\n",
      "Concatenate x.shape=TensorShape([None, 128, 38])\n",
      "TransformerEncoder x.shape=TensorShape([None, 128, 38])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "        d_k: int,\n",
    "        d_v: int,\n",
    "        n_heads: int,\n",
    "        d_model: int,\n",
    "        dropout: float = 0.0,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"Transformer encoder layer.\n",
    "\n",
    "        Based on the original concept proposed by Vaswani et al., 2017 (https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "        Args:\n",
    "            d_k (int): Key dimension (also used for Query dimension).\n",
    "            d_v (int): Value dimension.\n",
    "            n_heads (int): Number of attention heads.\n",
    "            d_model (int): Dimension of the feed forward sublayer.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_k = d_k # Query and Key have same dimension.\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads # Number of attention heads.\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, input_shape: tuple[tf.TensorShape,tf.TensorShape,tf.TensorShape]):\n",
    "\n",
    "        # First sublayer.\n",
    "        # Multi-head attention with add and norm.\n",
    "        self.attn_multi = keras.layers.MultiHeadAttention(\n",
    "            num_heads=self.n_heads,\n",
    "            key_dim=self.d_k,\n",
    "            value_dim=self.d_v,\n",
    "        )\n",
    "        self.attn_multi._build_from_signature(*input_shape)\n",
    "        self.attn_dropout = keras.layers.Dropout(rate=self.dropout)\n",
    "        self.attn_add = keras.layers.Add()\n",
    "        self.attn_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Second sublayer.\n",
    "        # Feed forward with add and norm.\n",
    "        d_query_feat = input_shape[0][-1] # Query feature size.\n",
    "        self.ff_dense_1 = keras.layers.Dense(units=self.d_model, activation='relu')\n",
    "        self.ff_dense_2 = keras.layers.Dense(units=d_query_feat)\n",
    "        self.ff_dropout = keras.layers.Dropout(rate=self.dropout)\n",
    "        self.ff_add = keras.layers.Add()\n",
    "        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x: tuple[tf.Tensor,tf.Tensor,tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"Encode input using multi-head self-attention mechanisms.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Tuple of Query, Value, and Key tensors. Note that the Key tensor is optional, if omitted the Value tensor will be used for both Key and Value.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor with shape (batch_size, sequence_length, embed_dim + 1)\n",
    "        \"\"\"\n",
    "        # x = (query, value, key)\n",
    "        # note that \"key\" is optional.\n",
    "\n",
    "        # First, do the attention sublayer.\n",
    "        x_attn = self.attn_multi(*x) # Unpack input as Query, Value, and optional Key.\n",
    "        x_attn = self.attn_dropout(x_attn)\n",
    "        x_attn = self.attn_add([x[0], x_attn]) # (residual) Add Query matrix with result of attention layer.\n",
    "        x_attn = self.attn_norm(x_attn) # Normalize the residual.\n",
    "\n",
    "        # Second, do the feed forward sublayer.\n",
    "        x_ff = self.ff_dense_1(x_attn)\n",
    "        x_ff = self.ff_dense_2(x_ff)\n",
    "        x_ff = self.ff_dropout(x_ff)\n",
    "        x_ff = self.ff_add([x_attn, x_ff])\n",
    "        x_ff = self.ff_norm(x_ff)\n",
    "\n",
    "        # Return output of feed forward sublayer.\n",
    "        return x_ff\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Retreive custom layer configuration for future loading.\n",
    "\n",
    "        Returns:\n",
    "            dict: Configuration dictionary.\n",
    "        \"\"\"\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_heads': self.n_heads,\n",
    "            'd_k': self.d_k,\n",
    "            'd_v': self.d_v,\n",
    "            'd_model': self.d_model,\n",
    "            'dropout': self.dropout,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "stock_feat = 5\n",
    "seq_len = 128\n",
    "embed_dim = 32\n",
    "d_k = 512\n",
    "d_v = 256\n",
    "n_heads = 8\n",
    "d_model = 512\n",
    "inp = keras.Input(shape=(seq_len, stock_feat))\n",
    "print(f\"{inp.shape=}\")\n",
    "x = Time2Vec(embed_dim)(inp)\n",
    "print(f\"Time2Vec {x.shape=}\")\n",
    "x = keras.layers.Concatenate(axis=-1)([inp, x])\n",
    "print(f\"Concatenate {x.shape=}\")\n",
    "x = TransformerEncoder(d_k, d_v, n_heads, d_model)([x, x, x])\n",
    "print(f\"TransformerEncoder {x.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e27c2da514be0f4555df3a4c15a4c6256ef40203ad64abea68e2343d203af1e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
