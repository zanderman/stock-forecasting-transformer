{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Forecasting using Transformers\n",
    "\n",
    "In this notebook we implement a Transformer model to forecast stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/a/64438413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import glob\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set() # Use seaborn themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "This section contains code that is modifies output path locations, random seed, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = Path('~/ml/datasets').expanduser()\n",
    "if not DATASET_ROOT.exists(): raise ValueError(f\"Dataset root directory does not exist at {DATASET_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds.\n",
    "SEED = 0\n",
    "tf.random.set_seed(SEED) # Only this works on ARC (since tensorflow==2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging (useful for ARC systems).\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) # Must be lowest of all handlers listed below.\n",
    "while logger.hasHandlers(): logger.removeHandler(logger.handlers[0]) # Clear all existing handlers.\n",
    "\n",
    "# Custom log formatting.\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "# Log to STDOUT (uses default formatting).\n",
    "sh = logging.StreamHandler(stream=sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "# Set Tensorflow logging level.\n",
    "tf.get_logger().setLevel('ERROR') # 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API configured\n"
     ]
    }
   ],
   "source": [
    "# Request user for Kaggle login if JSON file does not exist.\n",
    "kaggle_config_file = Path(\"~/.kaggle/kaggle.json\").expanduser()\n",
    "if not kaggle_config_file.exists() and os.environ.get(\"KAGGLE_USERNAME\", None) is None and os.environ.get(\"KAGGLE_KEY\", None) is None:\n",
    "    import json\n",
    "    import getpass\n",
    "    entry = getpass.getpass(prompt=\"Please enter your Kaggle username or JSON blob: \")\n",
    "    try:\n",
    "        blob = json.loads(entry)\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = blob['username']\n",
    "        os.environ[\"KAGGLE_KEY\"] = blob['key']\n",
    "    except:\n",
    "        api_key = getpass.getpass(prompt=\"Please enter your Kaggle API KEY: \")\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = entry\n",
    "        os.environ[\"KAGGLE_KEY\"] = api_key\n",
    "else:\n",
    "    logger.info('Kaggle API configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "# List all GPUs visible to TensorFlow.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "logger.info(f\"Num GPUs Available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    logger.info(f\"Name: {gpu.name}, Type: {gpu.device_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huge Stock Market Dataset from Kaggle\n",
    "\n",
    "https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HugeStockMarketDataset:\n",
    "    \"\"\"Wrapper for Huge Stock Market Dataset by Boris Marjanovic on Kaggle.\n",
    "\n",
    "    Source URL is: https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs/version/3\n",
    "\n",
    "    This class can be used like a Python dictionary, where keys are the stock/etf names, and values are\n",
    "    `pandas.DataFrame` objects corresponding to that stock/etf.\n",
    "    \"\"\"\n",
    "    root = 'HugeStockMarketDataset'\n",
    "\n",
    "    def __init__(self, \n",
    "        path: str,\n",
    "        files: list = None,\n",
    "        quiet: bool = False,\n",
    "        exclude_stocks: bool = False,\n",
    "        exclude_etfs: bool = False,\n",
    "        usecols: list[str] = ['Date','Open','High','Low','Close','Volume','OpenInt'],\n",
    "        ):\n",
    "        self.exclude_stocks = exclude_stocks\n",
    "        self.exclude_etfs = exclude_etfs\n",
    "        self.usecols = usecols\n",
    "        self._index = {}\n",
    "\n",
    "        # Download the dataset if necessary.\n",
    "        newpath = Path(path).expanduser()/self.root\n",
    "        if not newpath.exists():\n",
    "            self.download(newpath, files, quiet=quiet)\n",
    "        else:\n",
    "            self.path = newpath\n",
    "            self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"Creates an internal index of stocks and ETFs for lookup.\"\"\"\n",
    "\n",
    "        # Helper function to index a folder of files.\n",
    "        def _index_folder(dir: Path):\n",
    "            for file in glob.iglob(str(dir/'*.txt'), recursive=True):\n",
    "                filename = Path(file).name\n",
    "                product_name = filename.split('.', maxsplit=1)[0]\n",
    "                self._index[product_name] = file\n",
    "\n",
    "        # Index all stocks.\n",
    "        if not self.exclude_stocks:\n",
    "            _index_folder(self.path/'Stocks')\n",
    "        \n",
    "        # Index all ETFs.\n",
    "        if not self.exclude_etfs:\n",
    "            _index_folder(self.path/'ETFs')\n",
    "\n",
    "    def download(self, path: str, files: list = None, quiet: bool = True):\n",
    "        \"\"\"Downloads the dataset from Kaggle.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to place the download.\n",
    "            files (list, optional): Subset list of files to download instead of entire dataset. Defaults to None.\n",
    "            quiet (bool, optional): Suppress verbose output. Defaults to True.\n",
    "        \"\"\"\n",
    "        import kaggle\n",
    "        kaggle_dataset = 'borismarjanovic/price-volume-data-for-all-us-stocks-etfs'\n",
    "        kaggle.api.authenticate()\n",
    "\n",
    "        # Save the new downloaded path.\n",
    "        self.path = Path(path).expanduser()\n",
    "\n",
    "        # Specific file list was given.\n",
    "        if files is not None:\n",
    "            for f in files:\n",
    "                kaggle.api.dataset_download_file(\n",
    "                    dataset=kaggle_dataset,\n",
    "                    file_name=f,\n",
    "                    path=path/f,\n",
    "                    quiet=quiet,\n",
    "                )\n",
    "        # Download all files.\n",
    "        else:\n",
    "            kaggle.api.dataset_download_files(\n",
    "                dataset=kaggle_dataset,\n",
    "                path=path,\n",
    "                unzip=True,\n",
    "                quiet=quiet,\n",
    "            )\n",
    "\n",
    "        # Force rebuild the index after downloading.\n",
    "        logger.info(\"Building file index\")\n",
    "        self._build_index()\n",
    "\n",
    "    def get_dataframe(self, \n",
    "        key: str,\n",
    "        **kwargs,\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"Obtain historical data for stock or ETF in a pandas dataframe.\n",
    "\n",
    "        Optional keyword arguments are passed directly to `pandas.read_csv` function.\n",
    "\n",
    "        Args:\n",
    "            key (str): The identifier for the stock or ETF.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Historical data.\n",
    "        \"\"\"\n",
    "        return pd.read_csv(self._index[key], **kwargs)\n",
    "\n",
    "    #### Dictionary Override ######\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, str):\n",
    "            return self.get_dataframe(key, usecols=self.usecols)\n",
    "        elif isinstance(key, list):\n",
    "            return [self.get_dataframe(asset, usecols=self.usecols) for asset in key]\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        del self._index[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._index)\n",
    "\n",
    "    def items(self):\n",
    "        for key in self:\n",
    "            yield key, self.get_dataframe(key, usecols=self.usecols)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._index)\n",
    "\n",
    "    def keys(self):\n",
    "        \"\"\"Returns a list of all downloaded stocks and ETFs.\"\"\"\n",
    "        return self._index.keys()\n",
    "\n",
    "    ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration: https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing\n",
    "class WindowGenerator:\n",
    "    def __init__(self,\n",
    "        in_seq_len: int,\n",
    "        out_seq_len: int,\n",
    "        shift: int,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        in_feat: list[str] = None,\n",
    "        out_feat: list[str] = None,\n",
    "        batch_size: int = 32,\n",
    "        shuffle: bool = True,\n",
    "        ):\n",
    "        \"\"\"Constructs sliding windows of sequential data.\n",
    "\n",
    "        Data must already be split into train/val/test subsets,\n",
    "        and provided as `pandas.DataFrame` objects.\n",
    "\n",
    "        Args:\n",
    "            in_seq_len (int): Input sequence length.\n",
    "            out_seq_len (int): Output (target) sequence length.\n",
    "            shift (int): Number of indices to skip between elements when traversing window.\n",
    "            train_df (pd.DataFrame): Training data frame.\n",
    "            val_df (pd.DataFrame): Validation data frame.\n",
    "            test_df (pd.DataFrame): Testing data frame.\n",
    "            in_feat (list[str], optional): Desired subset of input features for window. Defaults to None.\n",
    "            out_feat (list[str], optional): Desired subset of output features for window. Defaults to None.\n",
    "            batch_size (int, optional): Batch size. Defaults to 32.\n",
    "            shuffle (bool, optional): Shuffle windows prior to batching. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Preserve dataframes.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Preserve sequence information.\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.shift = shift\n",
    "        self.total_window_len = in_seq_len + shift\n",
    "\n",
    "        # Setup indexing slices for window extraction.\n",
    "        self.in_slice = slice(0, self.in_seq_len)\n",
    "        self.out_slice = slice(self.total_window_len - self.out_seq_len, None)\n",
    "        self.in_idx = np.arange(self.total_window_len)[self.in_slice]\n",
    "        self.out_idx = np.arange(self.total_window_len)[self.out_slice]\n",
    "\n",
    "        # Setup train/val/test column extractors.\n",
    "        self.col2idx = {name: i for i, name in enumerate(train_df.columns)}\n",
    "        if in_feat is not None:\n",
    "            self.in_feat = in_feat\n",
    "            self.in_col_idx = [self.col2idx[col] for col in in_feat]\n",
    "        else:\n",
    "            self.in_col_idx = list(range(len(train_df.columns)))\n",
    "            self.in_feat = [train_df.columns[i] for i in self.in_col_idx]\n",
    "        if out_feat is not None:\n",
    "            self.out_feat = out_feat\n",
    "            self.out_col_idx = [self.col2idx[col] for col in out_feat]\n",
    "        else:\n",
    "            self.out_col_idx = list(range(len(train_df.columns)))\n",
    "            self.out_feat = [train_df.columns[i] for i in self.out_col_idx]\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"String representation of class.\"\"\"\n",
    "        return '\\n'.join([\n",
    "            f\"Total window length: {self.total_window_len}\",\n",
    "            f\"Input indices: {self.in_idx}\",\n",
    "            f\"Output indices: {self.out_idx}\",\n",
    "            f\"Input features: {self.in_feat}\",\n",
    "            f\"Output features: {self.out_feat}\",\n",
    "        ])\n",
    "\n",
    "    def split_window(self, \n",
    "        window: tf.Tensor, # window shape is (batch, seq, feat)\n",
    "        ) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"Splits a single window of data into input/output seqments.\n",
    "\n",
    "        Args:\n",
    "            window (tf.Tensor): Tensor of window data with shape (batch, seq, feat).\n",
    "\n",
    "        Returns:\n",
    "            tuple[tf.Tensor, tf.Tensor]: 2-tuple of input/output data segments, where the shapes are:\n",
    "                - Input window: (batch, in_seq_len, in_feat)\n",
    "                - Output window: (batch, out_seq_len, out_feat)\n",
    "        \"\"\"\n",
    "        # Decompose input/output sequence from given input window.\n",
    "        in_seq = tf.stack([window[:, self.in_slice, i] for i in self.in_col_idx], axis=-1)\n",
    "        out_seq = tf.stack([window[:, self.out_slice, i] for i in self.out_col_idx], axis=-1)\n",
    "\n",
    "        # Set shape for input/output sequences.\n",
    "        # Note that dimensions set to `None` are not updated.\n",
    "        in_seq = tf.ensure_shape(in_seq, (None, self.in_seq_len, None))\n",
    "        out_seq = tf.ensure_shape(out_seq, (None, self.out_seq_len, None))\n",
    "\n",
    "        return in_seq, out_seq\n",
    "\n",
    "    def make_dataset(self, \n",
    "        df: pd.DataFrame,\n",
    "        batch_size: int = 32,\n",
    "        shuffle: bool = True,\n",
    "        ) -> tf.data.Dataset:\n",
    "        \"\"\"Construct a TensorFlow Dataset from given input data frame.\n",
    "\n",
    "        Datasets load tuples of batched input/output windows with shapes:\n",
    "            - Input window: (batch, in_seq_len, in_feat)\n",
    "            - Output window: (batch, out_seq_len, out_feat)\n",
    "\n",
    "        Note that output windows are generally target sequences.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Source data frame.\n",
    "            batch_size (int, optional): Batch size. Defaults to 32.\n",
    "            shuffle (bool, optional): Shuffle windows prior to batching. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            tf.data.Dataset: Dataset object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert data frame into numpy matrix.\n",
    "        data = df.to_numpy()\n",
    "\n",
    "        # Convert data matrix into TensorFlow dataset.\n",
    "        dataset = keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_len,\n",
    "            sequence_stride=self.shift,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        # Pipe the raw dataset into the window splitting function.\n",
    "        dataset = dataset.map(self.split_window)\n",
    "\n",
    "        # Return the dataset.\n",
    "        return dataset\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        \"\"\"Training dataset.\"\"\"\n",
    "        return self.make_dataset(\n",
    "            df=self.train_df,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        \"\"\"Validation dataset.\"\"\"\n",
    "        return self.make_dataset(\n",
    "            df=self.val_df,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        \"\"\"Testing dataset.\"\"\"\n",
    "        return self.make_dataset(\n",
    "            df=self.test_df,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_dataset(\n",
    "    asset: str,\n",
    "    in_seq_len: int,\n",
    "    out_seq_len: int,\n",
    "    shift: int,\n",
    "    split: tuple[float, float, float], # must sum to 1.\n",
    "    in_feat: list[str] = ['Open','High','Low','Close','Volume'],\n",
    "    out_feat: list[str] = ['Open','High','Low','Close','Volume'],\n",
    "    normalize: bool = True,\n",
    "    return_datasets: bool = True,\n",
    "    return_df: bool = False,\n",
    "    return_window: bool = False,\n",
    "    ) -> None|tuple:\n",
    "    \"\"\"Load the history for a single stock within the Huge Stock Market Dataset.\n",
    "\n",
    "    Args:\n",
    "        asset (str): Name of stock or ETF to use.\n",
    "        in_seq_len (int): Input sequence length for window.\n",
    "        out_seq_len (int): Output (target) sequence length for window.\n",
    "        shift (int): Number of indices to skip when generating window.\n",
    "        split (tuple[float, float, float]): Tuple of `(train, val, test)` splits. Note that these must sum to `1`.\n",
    "        in_feat (list[str], optional): Desired subset of input features for window. Defaults to `['Open','High','Low','Close','Volume']`.\n",
    "        out_feat (list[str], optional): Desired subset of output features for window. Defaults to `['Open','High','Low','Close','Volume']`.\n",
    "        normalize (bool, optional): Normalize the data using mean/std method. Defaults to `True`.\n",
    "        return_datasets (bool, optional): Return train/val/test datasets. Defaults to `True`.\n",
    "        return_df (bool, optional): Return original data frame. Defaults to `False`.\n",
    "        return_window (bool, optional): Return window generator. Defaults to `False`.\n",
    "\n",
    "    Returns:\n",
    "        None|tuple: Several return options:\n",
    "            - `return_datasets=False`, `return_df=False`, `return_window=False`: Returns None.\n",
    "            - `return_datasets=True`, `return_df=False`, `return_window=False`: Returns tuple of `(train, val, test)` datasets.\n",
    "            - `return_datasets=False`, `return_df=True`, `return_window=False`: Returns the original dataset `df`.\n",
    "            - `return_datasets=False`, `return_df=False`, `return_window=True`: Returns the window generator `windowgen`.\n",
    "            - `return_datasets=True`, `return_df=True`, `return_window=False`: Returns tuple of `((train, val, test), df)`\n",
    "            - `return_datasets=True`, `return_df=False`, `return_window=True`: Returns tuple of `((train, val, test), windowgen)`\n",
    "            - `return_datasets=False`, `return_df=True`, `return_window=True`: Returns tuple of `(windowgen, df)`\n",
    "            - `return_datasets=True`, `return_df=True`, `return_window=True`: Returns tuple of `((train, val, test), df, windowgen)`\n",
    "    \"\"\"\n",
    "    np.testing.assert_almost_equal(sum(split), 1., err_msg='Split must sum to 1.')\n",
    "    train_split, _, test_split = split\n",
    "\n",
    "    # Get dataframe for desired asset.\n",
    "    dataset = HugeStockMarketDataset(DATASET_ROOT, usecols=['Date','Open','High','Low','Close','Volume'])\n",
    "    df = dataset[asset]\n",
    "\n",
    "    # Split dataframe into train/val/test dataframes.\n",
    "    # Inspiration: https://www.tensorflow.org/tutorials/structured_data/time_series#split_the_data\n",
    "    n = len(df.index) # Total number of data records.\n",
    "    df_train = df[:int(n*train_split)].copy()\n",
    "    df_val = df[int(n*train_split):int(n*(1-test_split))].copy()\n",
    "    df_test = df[int(n*(1-test_split)):].copy()\n",
    "\n",
    "    # Drop the 'Date' column, so that we only use the floating-point columns.\n",
    "    df_train.drop(columns=['Date'], inplace=True, errors='ignore')\n",
    "    df_val.drop(columns=['Date'], inplace=True, errors='ignore')\n",
    "    df_test.drop(columns=['Date'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Normalize the datasets using train-data statistics.\n",
    "    # Note that only the training data is used for statistics.\n",
    "    # Inspiration: https://www.tensorflow.org/tutorials/structured_data/time_series#normalize_the_data\n",
    "    if normalize:\n",
    "        train_mean = df_train.mean()\n",
    "        train_std = df_train.std()\n",
    "        df_train = (df_train - train_mean)/train_std\n",
    "        df_val = (df_val - train_mean)/train_std\n",
    "        df_test = (df_test - train_mean)/train_std\n",
    "\n",
    "    # Build window generator for datasets.\n",
    "    w = WindowGenerator(\n",
    "        in_seq_len=in_seq_len,\n",
    "        out_seq_len=out_seq_len,\n",
    "        shift=shift,\n",
    "        train_df=df_train,\n",
    "        val_df=df_val,\n",
    "        test_df=df_test,\n",
    "        in_feat=in_feat,\n",
    "        out_feat=out_feat,\n",
    "    )\n",
    "\n",
    "    # Extract elements for result list.\n",
    "    result = []\n",
    "    # Extract train/val/test datasets and return.\n",
    "    if return_datasets:\n",
    "        result.append((w.train, w.val, w.test))\n",
    "    # Extract the data frame.\n",
    "    if return_df:\n",
    "        result.append(df)\n",
    "    # Extract the window generator.\n",
    "    if return_window:\n",
    "        result.append(w)\n",
    "\n",
    "    # Return None if nothing was selected to return.\n",
    "    if len(result) == 0:\n",
    "        return None\n",
    "    # Return single element.\n",
    "    elif len(result) == 1:\n",
    "        return result[0]\n",
    "    # Return entire tuple of results.\n",
    "    else:\n",
    "        return tuple(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets actually load the dataset. In this case, we're only looking at the `aapl` stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/GoogleDrive/My Drive/Virginia Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb Cell 16'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=0'>1</a>\u001b[0m asset \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39maapl\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=1'>2</a>\u001b[0m (dataset_train, dataset_val, dataset_test), df \u001b[39m=\u001b[39m load_stock_dataset(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=2'>3</a>\u001b[0m     asset\u001b[39m=\u001b[39;49masset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=3'>4</a>\u001b[0m     in_seq_len\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=4'>5</a>\u001b[0m     out_seq_len\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=5'>6</a>\u001b[0m     shift\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=6'>7</a>\u001b[0m     split\u001b[39m=\u001b[39;49m(\u001b[39m0.7\u001b[39;49m, \u001b[39m0.2\u001b[39;49m, \u001b[39m0.1\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=7'>8</a>\u001b[0m     return_datasets\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=8'>9</a>\u001b[0m     return_df\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_train\u001b[39m.\u001b[39melement_spec\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000029?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_val\u001b[39m.\u001b[39melement_spec\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Volumes/GoogleDrive/My Drive/Virginia Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb Cell 14'\u001b[0m in \u001b[0;36mload_stock_dataset\u001b[0;34m(asset, in_seq_len, out_seq_len, shift, split, in_feat, out_feat, normalize, return_datasets, return_df, return_window)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000019?line=78'>79</a>\u001b[0m \u001b[39m# Extract train/val/test datasets and return.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000019?line=79'>80</a>\u001b[0m \u001b[39mif\u001b[39;00m return_datasets:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000019?line=80'>81</a>\u001b[0m     result\u001b[39m.\u001b[39mappend((w\u001b[39m.\u001b[39;49mtrain, w\u001b[39m.\u001b[39mval, w\u001b[39m.\u001b[39mtest))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000019?line=81'>82</a>\u001b[0m \u001b[39m# Extract the data frame.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000019?line=82'>83</a>\u001b[0m \u001b[39mif\u001b[39;00m return_df:\n",
      "\u001b[1;32m/Volumes/GoogleDrive/My Drive/Virginia Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb Cell 13'\u001b[0m in \u001b[0;36mWindowGenerator.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=141'>142</a>\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=142'>143</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=143'>144</a>\u001b[0m     \u001b[39m\"\"\"Training dataset.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=144'>145</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_dataset(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=145'>146</a>\u001b[0m         df\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_df,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=146'>147</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=147'>148</a>\u001b[0m         shuffle\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshuffle,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=148'>149</a>\u001b[0m     )\n",
      "\u001b[1;32m/Volumes/GoogleDrive/My Drive/Virginia Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb Cell 13'\u001b[0m in \u001b[0;36mWindowGenerator.make_dataset\u001b[0;34m(self, df, batch_size, shuffle)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=123'>124</a>\u001b[0m data \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=125'>126</a>\u001b[0m \u001b[39m# Convert data matrix into TensorFlow dataset.\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=126'>127</a>\u001b[0m dataset \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mtimeseries_dataset_from_array(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=127'>128</a>\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=128'>129</a>\u001b[0m     targets\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=129'>130</a>\u001b[0m     sequence_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtotal_window_len,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=130'>131</a>\u001b[0m     sequence_stride\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshift,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=131'>132</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=132'>133</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=133'>134</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=135'>136</a>\u001b[0m \u001b[39m# Pipe the raw dataset into the window splitting function.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/courses/2022_spring/ece_6524/assignments/final_project/stock-forecasting-transformer/forecast_transformer.ipynb#ch0000018?line=136'>137</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_window)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py:218\u001b[0m, in \u001b[0;36mtimeseries_dataset_from_array\u001b[0;34m(data, targets, sequence_length, sequence_stride, sampling_rate, batch_size, shuffle, seed, start_index, end_index)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=208'>209</a>\u001b[0m \u001b[39m# For each initial window position, generates indices of the window elements\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=209'>210</a>\u001b[0m indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mzip(\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=210'>211</a>\u001b[0m     (tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mrange(\u001b[39mlen\u001b[39m(start_positions)), positions_ds))\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=211'>212</a>\u001b[0m         \u001b[39mlambda\u001b[39;00m i, positions: tf\u001b[39m.\u001b[39mrange(  \u001b[39m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=214'>215</a>\u001b[0m             sampling_rate),\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=215'>216</a>\u001b[0m         num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=217'>218</a>\u001b[0m dataset \u001b[39m=\u001b[39m sequences_from_indices(data, indices, start_index, end_index)\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=218'>219</a>\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=219'>220</a>\u001b[0m   indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mzip(\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=220'>221</a>\u001b[0m       (tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mrange(\u001b[39mlen\u001b[39m(start_positions)), positions_ds))\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=221'>222</a>\u001b[0m           \u001b[39mlambda\u001b[39;00m i, positions: positions[i],\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=222'>223</a>\u001b[0m           num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py:235\u001b[0m, in \u001b[0;36msequences_from_indices\u001b[0;34m(array, indices_ds, start_index, end_index)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=233'>234</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msequences_from_indices\u001b[39m(array, indices_ds, start_index, end_index):\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=234'>235</a>\u001b[0m   dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensors(array[start_index : end_index])\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=235'>236</a>\u001b[0m   dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mzip((dataset\u001b[39m.\u001b[39mrepeat(), indices_ds))\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=236'>237</a>\u001b[0m       \u001b[39mlambda\u001b[39;00m steps, inds: tf\u001b[39m.\u001b[39mgather(steps, inds),  \u001b[39m# pylint: disable=unnecessary-lambda\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=237'>238</a>\u001b[0m       num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/preprocessing/timeseries.py?line=238'>239</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:701\u001b[0m, in \u001b[0;36mDatasetV2.from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=663'>664</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=664'>665</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensors\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=665'>666</a>\u001b[0m   \u001b[39m\"\"\"Creates a `Dataset` with a single element, comprising the given tensors.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=666'>667</a>\u001b[0m \n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=667'>668</a>\u001b[0m \u001b[39m  `from_tensors` produces a dataset containing only a single element. To slice\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=698'>699</a>\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=699'>700</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=700'>701</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4636\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4633'>4634</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, element, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4634'>4635</a>\u001b[0m   \u001b[39m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4635'>4636</a>\u001b[0m   element \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39;49mnormalize_element(element)\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4636'>4637</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4637'>4638</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure, element)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:129\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py?line=125'>126</a>\u001b[0m       \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py?line=126'>127</a>\u001b[0m         dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(spec, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py?line=127'>128</a>\u001b[0m         normalized_components\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py?line=128'>129</a>\u001b[0m             ops\u001b[39m.\u001b[39;49mconvert_to_tensor(t, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcomponent_\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m i, dtype\u001b[39m=\u001b[39;49mdtype))\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py?line=129'>130</a>\u001b[0m \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py?line=160'>161</a>\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py?line=161'>162</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py?line=162'>163</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1621\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=1615'>1616</a>\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=1616'>1617</a>\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=1617'>1618</a>\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=1619'>1620</a>\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=1620'>1621</a>\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=1622'>1623</a>\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/ops.py?line=1623'>1624</a>\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:52\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py?line=49'>50</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py?line=50'>51</a>\u001b[0m   \u001b[39mdel\u001b[39;00m as_ref  \u001b[39m# Unused.\u001b[39;00m\n\u001b[0;32m---> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py?line=51'>52</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m constant_op\u001b[39m.\u001b[39;49mconstant(value, dtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=173'>174</a>\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=174'>175</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=175'>176</a>\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=176'>177</a>\u001b[0m \n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=177'>178</a>\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=268'>269</a>\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=269'>270</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=270'>271</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=271'>272</a>\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:283\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=280'>281</a>\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=281'>282</a>\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=282'>283</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=284'>285</a>\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=285'>286</a>\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:308\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=305'>306</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=306'>307</a>\u001b[0m   \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=307'>308</a>\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=308'>309</a>\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=309'>310</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=103'>104</a>\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=104'>105</a>\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py?line=105'>106</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "asset = 'aapl'\n",
    "(dataset_train, dataset_val, dataset_test), df = load_stock_dataset(\n",
    "    asset=asset,\n",
    "    in_seq_len=128,\n",
    "    out_seq_len=1,\n",
    "    shift=1,\n",
    "    split=(0.7, 0.2, 0.1),\n",
    "    return_datasets=True,\n",
    "    return_df=True,\n",
    ")\n",
    "print(f\"{dataset_train.element_spec=}\")\n",
    "print(f\"{dataset_val.element_spec=}\")\n",
    "print(f\"{dataset_test.element_spec=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the closing price and volume for the chosen stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "fig.suptitle(f'{asset.upper()} Close Price and Volume')\n",
    "\n",
    "n = len(df.index)//8 # Number of date record ticks on X-axis.\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "sns.lineplot(x='Date', y='Close', data=df, ax=ax)\n",
    "ax.set_xticks(range(0, df.shape[0], n))\n",
    "ax.set_xticklabels(df['Date'].loc[::n])\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "sns.lineplot(x='Date', y='Volume', data=df, ax=ax)\n",
    "ax.set_xticks(range(0, df.shape[0], n))\n",
    "ax.set_xticklabels(df['Date'].loc[::n]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time2Vec Embedding\n",
    "\n",
    "https://arxiv.org/abs/1907.05321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp.shape=TensorShape([None, 128, 5])\n",
      "x.shape=TensorShape([None, 128, 33])\n",
      "x.shape=TensorShape([None, 128, 38])\n"
     ]
    }
   ],
   "source": [
    "class Time2Vec(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim: int, activation: str = 'sin', **kwargs):\n",
    "        \"\"\"Vector embedding representation of time.\n",
    "\n",
    "        Based on the original concept proposed by Kazemi et al., 2019 (https://arxiv.org/abs/1907.05321).\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Length of the time embedding vector.\n",
    "            activation (str, optional): Periodic activation function. Possible values are ['sin', 'cos']. Defaults to 'sin'.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.activation = activation.lower() # Convert to lower-case.\n",
    "\n",
    "        # Set periodic activation function.\n",
    "        if self.activation.startswith('sin'):\n",
    "            self.activation_func = tf.sin\n",
    "        elif self.activation.startswith('cos'):\n",
    "            self.activation_func = tf.cos\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported periodic activation function \"{activation}\"')\n",
    "\n",
    "    def build(self, input_shape: list[int]):\n",
    "\n",
    "        # Weight and bias term for linear portion (i = 0)\n",
    "        # of embedding.\n",
    "        self.w_linear = self.add_weight(\n",
    "            name='w_linear',\n",
    "            shape=(input_shape[1], 1,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b_linear = self.add_weight(\n",
    "            name='b_linear',\n",
    "            shape=(input_shape[1], 1,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Weight and bias terms for the periodic\n",
    "        # portion (1 <= i <= k) of embedding.\n",
    "        self.w_periodic = self.add_weight(\n",
    "            name='w_periodic',\n",
    "            shape=(input_shape[-1], self.embed_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b_periodic = self.add_weight(\n",
    "            name='b_periodic',\n",
    "            shape=(input_shape[1], self.embed_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Embed input into linear and periodic feature components.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor with shape (batch_size, sequence_length, feature_size)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor with shape (batch_size, sequence_length, embed_dim + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Linear term (i = 0).\n",
    "        embed_linear = tf.tensordot(x, self.w_linear, axes=1) + self.b_linear\n",
    "\n",
    "        # Periodic terms (1 <= i <= k).\n",
    "        inner = tf.tensordot(x, self.w_periodic, axes=1) + self.b_periodic\n",
    "        embed_periodic = self.activation_func(inner)\n",
    "\n",
    "        # Return concatenated linear and periodic features.\n",
    "        return tf.concat([embed_linear, embed_periodic], axis=-1)\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Retreive custom layer configuration for future loading.\n",
    "\n",
    "        Returns:\n",
    "            dict: Configuration dictionary.\n",
    "        \"\"\"\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "stock_feat = 5\n",
    "seq_len = 128\n",
    "embed_dim = 32\n",
    "inp = keras.Input(shape=(seq_len, stock_feat))\n",
    "logger.info(f\"{inp.shape=}\")\n",
    "x = Time2Vec(embed_dim)(inp)\n",
    "logger.info(f\"{x.shape=}\")\n",
    "x = keras.layers.Concatenate(axis=-1)([inp, x])\n",
    "logger.info(f\"{x.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layers\n",
    "\n",
    "Currently uses attention layers provided by TensorFlow. See https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Layer\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/transformer#encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp.shape=TensorShape([None, 128, 5])\n",
      "Time2Vec x.shape=TensorShape([None, 128, 33])\n",
      "Concatenate x.shape=TensorShape([None, 128, 38])\n",
      "TransformerEncoder x.shape=TensorShape([None, 128, 38])\n",
      "TransformerEncoderLayer x.shape=TensorShape([None, 128, 38])\n",
      "TransformerEncoderLayer x.shape=TensorShape([None, 128, 38])\n",
      "GlobalAvgPool1D x.shape=TensorShape([None, 128])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderLayer(keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "        d_k: int,\n",
    "        d_v: int,\n",
    "        n_heads: int,\n",
    "        d_ff: int,\n",
    "        dropout: float = 0.0,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"Transformer encoder layer.\n",
    "\n",
    "        Based on the original concept proposed by Vaswani et al., 2017 (https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "        Args:\n",
    "            d_k (int): Key dimension (also used for Query dimension).\n",
    "            d_v (int): Value dimension.\n",
    "            n_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimension of the feed forward sublayer.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_k = d_k # Query and Key have same dimension.\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads # Number of attention heads.\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, input_shape: tuple[tf.TensorShape,tf.TensorShape,tf.TensorShape]):\n",
    "\n",
    "        # First sublayer.\n",
    "        # Multi-head attention with add and norm.\n",
    "        self.attn_multi = keras.layers.MultiHeadAttention(\n",
    "            num_heads=self.n_heads,\n",
    "            key_dim=self.d_k,\n",
    "            value_dim=self.d_v,\n",
    "        )\n",
    "        self.attn_multi._build_from_signature(*input_shape)\n",
    "        self.attn_dropout = keras.layers.Dropout(rate=self.dropout)\n",
    "        self.attn_add = keras.layers.Add()\n",
    "        self.attn_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Second sublayer.\n",
    "        # Feed forward with add and norm.\n",
    "        d_query_feat = input_shape[0][-1] # Query feature size.\n",
    "        self.ff_dense_1 = keras.layers.Dense(units=self.d_ff, activation='relu')\n",
    "        self.ff_dense_2 = keras.layers.Dense(units=d_query_feat)\n",
    "        self.ff_dropout = keras.layers.Dropout(rate=self.dropout)\n",
    "        self.ff_add = keras.layers.Add()\n",
    "        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x: tuple[tf.Tensor,tf.Tensor,tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"Encode input using multi-head self-attention mechanisms.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Tuple of Query, Value, and Key tensors. Note that the Key tensor is optional, if omitted the Value tensor will be used for both Key and Value.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor with shape (batch_size, sequence_length, embed_dim + 1)\n",
    "        \"\"\"\n",
    "        # x = (query, value, key)\n",
    "        # note that \"key\" is optional.\n",
    "\n",
    "        # First, do the attention sublayer.\n",
    "        x_attn = self.attn_multi(*x) # Unpack input as Query, Value, and optional Key.\n",
    "        x_attn = self.attn_dropout(x_attn)\n",
    "        x_attn = self.attn_add([x[0], x_attn]) # (residual) Add Query matrix with result of attention layer.\n",
    "        x_attn = self.attn_norm(x_attn) # Normalize the residual.\n",
    "\n",
    "        # Second, do the feed forward sublayer.\n",
    "        x_ff = self.ff_dense_1(x_attn)\n",
    "        x_ff = self.ff_dense_2(x_ff)\n",
    "        x_ff = self.ff_dropout(x_ff)\n",
    "        x_ff = self.ff_add([x_attn, x_ff])\n",
    "        x_ff = self.ff_norm(x_ff)\n",
    "\n",
    "        # Return output of feed forward sublayer.\n",
    "        return x_ff\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"Retreive custom layer configuration for future loading.\n",
    "\n",
    "        Returns:\n",
    "            dict: Configuration dictionary.\n",
    "        \"\"\"\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_heads': self.n_heads,\n",
    "            'd_k': self.d_k,\n",
    "            'd_v': self.d_v,\n",
    "            'd_ff': self.d_ff,\n",
    "            'dropout': self.dropout,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "stock_feat = 5\n",
    "seq_len = 128\n",
    "embed_dim = 32\n",
    "d_k = 512\n",
    "d_v = 256\n",
    "n_heads = 8\n",
    "d_ff = 512\n",
    "inp = keras.Input(shape=(seq_len, stock_feat))\n",
    "logger.info(f\"{inp.shape=}\")\n",
    "x = Time2Vec(embed_dim)(inp)\n",
    "logger.info(f\"Time2Vec {x.shape=}\")\n",
    "x = keras.layers.Concatenate(axis=-1)([inp, x])\n",
    "logger.info(f\"Concatenate {x.shape=}\")\n",
    "x = TransformerEncoderLayer(d_k, d_v, n_heads, d_ff)([x, x, x])\n",
    "logger.info(f\"TransformerEncoder {x.shape=}\")\n",
    "x = TransformerEncoderLayer(d_k, d_v, n_heads, d_ff)([x, x, x])\n",
    "logger.info(f\"TransformerEncoderLayer {x.shape=}\")\n",
    "x = TransformerEncoderLayer(d_k, d_v, n_heads, d_ff)([x, x, x])\n",
    "logger.info(f\"TransformerEncoderLayer {x.shape=}\")\n",
    "x = keras.layers.GlobalAvgPool1D(data_format='channels_first')(x)\n",
    "logger.info(f\"GlobalAvgPool1D {x.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 128, 5)]     0           []                               \n",
      "                                                                                                  \n",
      " time2_vec_2 (Time2Vec)         (None, 128, 33)      4512        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 38)      0           ['input_3[0][0]',                \n",
      "                                                                  'time2_vec_2[0][0]']            \n",
      "                                                                                                  \n",
      " transformer_encoder_layer_3 (T  (None, 128, 38)     516836      ['concatenate_2[0][0]',          \n",
      " ransformerEncoderLayer)                                          'concatenate_2[0][0]',          \n",
      "                                                                  'concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " transformer_encoder_layer_4 (T  (None, 128, 38)     516836      ['transformer_encoder_layer_3[0][\n",
      " ransformerEncoderLayer)                                         0]',                             \n",
      "                                                                  'transformer_encoder_layer_3[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'transformer_encoder_layer_3[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " transformer_encoder_layer_5 (T  (None, 128, 38)     516836      ['transformer_encoder_layer_4[0][\n",
      " ransformerEncoderLayer)                                         0]',                             \n",
      "                                                                  'transformer_encoder_layer_4[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'transformer_encoder_layer_4[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 128)         0           ['transformer_encoder_layer_5[0][\n",
      " obalAveragePooling1D)                                           0]']                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           4160        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3)            195         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,567,631\n",
      "Trainable params: 1,567,631\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(\n",
    "    in_seq_len: int,\n",
    "    in_feat: int,\n",
    "    out_feat: int,\n",
    "    fc_units: list[int], # list of fully-connected dimensions before classifier.\n",
    "    embed_dim: int,\n",
    "    d_k: int,\n",
    "    d_v: int,\n",
    "    n_heads: int,\n",
    "    d_ff: int,\n",
    "    dropout: float = 0.0,\n",
    "    n_encoders: int = 3,\n",
    "    ):\n",
    "\n",
    "    # Input sequence of features.\n",
    "    inp = keras.Input(shape=(in_seq_len, in_feat))\n",
    "    # Time embedding.\n",
    "    x = Time2Vec(embed_dim)(inp)\n",
    "    # Combine input with embedding to form attention input features.\n",
    "    x = keras.layers.Concatenate(axis=-1)([inp, x])\n",
    "    # Pass combined featured through cascaded self-attention encoder sublayers.\n",
    "    for _ in range(n_encoders):\n",
    "        x = TransformerEncoderLayer(\n",
    "            d_k=d_k,\n",
    "            d_v=d_v,\n",
    "            n_heads=n_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout,\n",
    "        )((x, x, x)) # (query, value, key)\n",
    "    # Downsample to the original sequence dimension.\n",
    "    x = keras.layers.GlobalAvgPool1D(data_format='channels_first')(x) # shape=(in_seq_len,)\n",
    "    x = keras.layers.Dropout(rate=dropout)(x)\n",
    "    # Fully-connected network before classifier.\n",
    "    for units in fc_units: \n",
    "        x = keras.layers.Dense(units=units, activation='relu')(x)\n",
    "        x = keras.layers.Dropout(rate=dropout)(x)\n",
    "    # Classifier.\n",
    "    x = keras.layers.Dense(units=out_feat, activation='linear')(x)\n",
    "\n",
    "    # Construct model class and return.\n",
    "    return keras.Model(inputs=inp, outputs=x)\n",
    "\n",
    "\n",
    "kwargs = dict(\n",
    "    in_seq_len=128, # Number of days in the past.\n",
    "    in_feat=5, # Number of features for each day in the past.\n",
    "    out_feat=3, # Number of features on 1-day horizon.\n",
    "    fc_units=[64,64],\n",
    "    embed_dim=32,\n",
    "    d_k=512,\n",
    "    d_v=256,\n",
    "    n_heads=8,\n",
    "    d_ff=512,\n",
    ")\n",
    "model = build_model(**kwargs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e27c2da514be0f4555df3a4c15a4c6256ef40203ad64abea68e2343d203af1e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
